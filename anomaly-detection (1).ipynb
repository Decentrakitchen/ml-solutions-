{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13044767,"sourceType":"datasetVersion","datasetId":8260226}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.signal import savgol_filter\nimport warnings\nwarnings.filterwarnings('ignore')\nimport joblib\nimport pickle\nimport os\nimport json\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass LSTMAutoencoder(nn.Module):\n    \"\"\"\n    LSTM Autoencoder for temporal anomaly detection in PyTorch\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=64, latent_dim=10, num_layers=2, sequence_length=20):\n        super(LSTMAutoencoder, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        self.num_layers = num_layers\n        self.sequence_length = sequence_length\n        \n        # Encoder\n        self.encoder_lstm = nn.LSTM(\n            input_dim, hidden_dim, num_layers, \n            batch_first=True, dropout=0.2 if num_layers > 1 else 0\n        )\n        self.encoder_fc = nn.Linear(hidden_dim, latent_dim)\n        \n        # Decoder\n        self.decoder_fc = nn.Linear(latent_dim, hidden_dim)\n        self.decoder_lstm = nn.LSTM(\n            hidden_dim, hidden_dim, num_layers, \n            batch_first=True, dropout=0.2 if num_layers > 1 else 0\n        )\n        self.output_projection = nn.Linear(hidden_dim, input_dim)\n        \n        self.dropout = nn.Dropout(0.2)\n        \n    def encode(self, x):\n        # x shape: (batch_size, sequence_length, input_dim)\n        lstm_out, (hidden, cell) = self.encoder_lstm(x)\n        # Use the last hidden state\n        encoded = self.encoder_fc(hidden[-1])  # Shape: (batch_size, latent_dim)\n        return encoded\n    \n    def decode(self, encoded):\n        batch_size = encoded.size(0)\n        \n        # Project to hidden dimension\n        decoded = self.decoder_fc(encoded)  # Shape: (batch_size, hidden_dim)\n        \n        # Repeat for sequence length\n        decoded = decoded.unsqueeze(1).repeat(1, self.sequence_length, 1)  # Shape: (batch_size, seq_len, hidden_dim)\n        \n        # Pass through LSTM\n        lstm_out, _ = self.decoder_lstm(decoded)\n        \n        # Project to output dimension\n        output = self.output_projection(lstm_out)  # Shape: (batch_size, seq_len, input_dim)\n        \n        return output\n    \n    def forward(self, x):\n        encoded = self.encode(x)\n        decoded = self.decode(encoded)\n        return decoded\n\nclass SequenceDataset(Dataset):\n    \"\"\"\n    Custom Dataset for sequence data\n    \"\"\"\n    \n    def __init__(self, sequences, labels=None):\n        self.sequences = torch.FloatTensor(sequences)\n        self.labels = torch.FloatTensor(labels) if labels is not None else None\n        \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return self.sequences[idx], self.labels[idx]\n        return self.sequences[idx]\n\nclass EarlyStopping:\n    \"\"\"\n    Early stopping utility class\n    \"\"\"\n    \n    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n            \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\nclass DrivingAnomalyDetectorPyTorchFixed:\n    \"\"\"\n    FIXED: Advanced anomaly detection system for dangerous driving behaviors using PyTorch\n    \"\"\"\n    \n    def __init__(self, df):\n        \"\"\"\n        Initialize with geotrack data\n        \n        Args:\n            df: DataFrame with columns: randomized_id, lat, lng, alt, spd, azm\n        \"\"\"\n        self.df = df.copy()\n        self.features_df = None\n        self.scaler = RobustScaler()\n        \n        # Models\n        self.isolation_forest = None\n        self.one_class_svm = None\n        self.lstm_autoencoder = None\n        \n        # Store reconstruction threshold\n        self.lstm_threshold = None\n        \n    def safe_divide(self, numerator, denominator, default_value=0.0):\n        \"\"\"Safe division to avoid NaN and infinite values\"\"\"\n        with np.errstate(divide='ignore', invalid='ignore'):\n            result = np.divide(numerator, denominator)\n            result = np.where(np.isfinite(result), result, default_value)\n            return result\n    \n    def calculate_driving_physics(self):\n        \"\"\"\n        FIXED: Calculate advanced physics-based features for driving analysis\n        \"\"\"\n        print(\"Calculating driving physics features...\")\n        \n        # Sort by trip and sequence\n        self.df = self.df.sort_values(['randomized_id', 'lat', 'lng'])\n        self.df['sequence'] = self.df.groupby('randomized_id').cumcount()\n        \n        # Time estimation (assuming 1-second intervals)\n        self.df['time_delta'] = 1.0  # seconds\n        \n        def calculate_trip_features(group):\n            \"\"\"Calculate features for each trip with NaN/inf protection\"\"\"\n            if len(group) < 3:\n                # Fill with safe default values for short trips\n                group['distance'] = 0.0\n                group['speed_smooth'] = group['spd']\n                group['acceleration'] = 0.0\n                group['jerk'] = 0.0\n                group['angular_velocity'] = 0.0\n                group['lateral_acceleration'] = 0.0\n                group['heading_change_rate'] = 0.0\n                group['curvature'] = 0.0\n                return group\n            \n            # Distance calculation using Haversine formula\n            def haversine_distance(lat1, lon1, lat2, lon2):\n                R = 6371000  # Earth's radius in meters\n                lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n                dlat = lat2 - lat1\n                dlon = lon2 - lon1\n                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n                c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))  # Clip to avoid numerical errors\n                return R * c\n            \n            # Calculate distances\n            distances = [0]  # First point\n            for i in range(1, len(group)):\n                try:\n                    dist = haversine_distance(\n                        group.iloc[i-1]['lat'], group.iloc[i-1]['lng'],\n                        group.iloc[i]['lat'], group.iloc[i]['lng']\n                    )\n                    # Limit maximum distance to avoid GPS errors\n                    dist = min(dist, 1000)  # Max 1km between consecutive points\n                    distances.append(dist)\n                except:\n                    distances.append(0)\n            \n            group['distance'] = distances\n            \n            # Smooth speed data to reduce GPS noise\n            if len(group) >= 5:\n                try:\n                    group['speed_smooth'] = savgol_filter(group['spd'], 5, 2)\n                except:\n                    group['speed_smooth'] = group['spd']\n            else:\n                group['speed_smooth'] = group['spd']\n            \n            # Ensure positive speeds\n            group['speed_smooth'] = np.maximum(group['speed_smooth'], 0)\n            \n            # Calculate acceleration (m/s¬≤) with error handling\n            speed_ms = group['speed_smooth'] / 3.6  # Convert km/h to m/s\n            try:\n                acceleration = np.gradient(speed_ms, group['time_delta'])\n                # Limit extreme accelerations\n                acceleration = np.clip(acceleration, -15, 15)  # Reasonable limits for cars\n            except:\n                acceleration = np.zeros(len(group))\n            group['acceleration'] = acceleration\n            \n            # Calculate jerk (m/s¬≥) with error handling\n            try:\n                jerk = np.gradient(acceleration, group['time_delta'])\n                jerk = np.clip(jerk, -20, 20)  # Reasonable limits for jerk\n            except:\n                jerk = np.zeros(len(group))\n            group['jerk'] = jerk\n            \n            # Calculate angular velocity (rad/s) with error handling\n            try:\n                azimuth_rad = np.radians(group['azm'])\n                azimuth_unwrapped = np.unwrap(azimuth_rad)\n                angular_velocity = np.gradient(azimuth_unwrapped, group['time_delta'])\n                angular_velocity = np.clip(angular_velocity, -np.pi, np.pi)  # Reasonable limits\n            except:\n                angular_velocity = np.zeros(len(group))\n            group['angular_velocity'] = angular_velocity\n            \n            # Calculate lateral acceleration (cornering force)\n            lateral_acceleration = speed_ms * angular_velocity\n            lateral_acceleration = np.clip(lateral_acceleration, -20, 20)  # Reasonable limits\n            group['lateral_acceleration'] = lateral_acceleration\n            \n            # Calculate heading change rate\n            heading_change = np.abs(angular_velocity)\n            group['heading_change_rate'] = heading_change\n            \n            # Calculate curvature (1/radius of turn) with safe division\n            group['curvature'] = self.safe_divide(\n                np.abs(angular_velocity), \n                speed_ms + 0.1,  # Add small value to avoid division by zero\n                default_value=0.0\n            )\n            \n            return group\n        \n        self.df = self.df.groupby('randomized_id').apply(calculate_trip_features)\n        self.df = self.df.reset_index(drop=True)\n        \n        # Final check for any remaining NaN or inf values\n        numeric_columns = ['distance', 'speed_smooth', 'acceleration', 'jerk', \n                          'angular_velocity', 'lateral_acceleration', 'heading_change_rate', 'curvature']\n        \n        for col in numeric_columns:\n            if col in self.df.columns:\n                self.df[col] = self.df[col].fillna(0)\n                self.df[col] = self.df[col].replace([np.inf, -np.inf], 0)\n        \n        print(\"Physics features calculated successfully\")\n        return self.df\n    \n    def engineer_anomaly_features(self):\n        \"\"\"\n        FIXED: Engineer specific features for anomaly detection with NaN/inf protection\n        \"\"\"\n        print(\"Engineering anomaly detection features...\")\n        \n        if 'acceleration' not in self.df.columns:\n            self.calculate_driving_physics()\n        \n        # Rolling window statistics for pattern detection\n        window_sizes = [3, 5, 10]\n        \n        for window in window_sizes:\n            # Speed patterns with error handling\n            try:\n                self.df[f'speed_std_{window}'] = self.df.groupby('randomized_id')['spd'].rolling(window, center=True, min_periods=1).std().reset_index(0, drop=True).fillna(0)\n                self.df[f'speed_max_{window}'] = self.df.groupby('randomized_id')['spd'].rolling(window, center=True, min_periods=1).max().reset_index(0, drop=True).fillna(0)\n                self.df[f'speed_min_{window}'] = self.df.groupby('randomized_id')['spd'].rolling(window, center=True, min_periods=1).min().reset_index(0, drop=True).fillna(0)\n            except:\n                self.df[f'speed_std_{window}'] = 0\n                self.df[f'speed_max_{window}'] = self.df['spd']\n                self.df[f'speed_min_{window}'] = self.df['spd']\n            \n            # Acceleration patterns with error handling\n            try:\n                self.df[f'accel_std_{window}'] = self.df.groupby('randomized_id')['acceleration'].rolling(window, center=True, min_periods=1).std().reset_index(0, drop=True).fillna(0)\n                self.df[f'accel_max_{window}'] = self.df.groupby('randomized_id')['acceleration'].rolling(window, center=True, min_periods=1).max().reset_index(0, drop=True).fillna(0)\n                self.df[f'accel_min_{window}'] = self.df.groupby('randomized_id')['acceleration'].rolling(window, center=True, min_periods=1).min().reset_index(0, drop=True).fillna(0)\n            except:\n                self.df[f'accel_std_{window}'] = 0\n                self.df[f'accel_max_{window}'] = self.df['acceleration']\n                self.df[f'accel_min_{window}'] = self.df['acceleration']\n        \n        # Extreme behavior indicators with safe thresholds\n        self.df['hard_braking'] = (self.df['acceleration'] < -4.0).astype(int)\n        self.df['hard_acceleration'] = (self.df['acceleration'] > 3.0).astype(int)\n        self.df['excessive_speed'] = (self.df['spd'] > 80).astype(int)\n        self.df['sharp_turn'] = (np.abs(self.df['lateral_acceleration']) > 4.0).astype(int)\n        self.df['erratic_steering'] = (np.abs(self.df['heading_change_rate']) > 0.5).astype(int)\n        \n        # Composite risk scores with safe division and clipping\n        self.df['acceleration_risk'] = np.clip(np.abs(self.df['acceleration']) / 10.0, 0, 1)\n        self.df['jerk_risk'] = np.clip(np.abs(self.df['jerk']) / 5.0, 0, 1)\n        self.df['lateral_risk'] = np.clip(np.abs(self.df['lateral_acceleration']) / 8.0, 0, 1)\n        self.df['speed_risk'] = np.clip(np.maximum(0, (self.df['spd'] - 60) / 40.0), 0, 1)\n        \n        # Overall risk score with weights summing to 1\n        self.df['overall_risk'] = (\n            self.df['acceleration_risk'] * 0.25 +\n            self.df['jerk_risk'] * 0.20 +\n            self.df['lateral_risk'] * 0.25 +\n            self.df['speed_risk'] * 0.15 +\n            (self.df['hard_braking'] + self.df['hard_acceleration'] + \n             self.df['sharp_turn'] + self.df['erratic_steering']) * 0.15 / 4  # Normalize by 4\n        )\n        \n        # Ensure overall risk is between 0 and 1\n        self.df['overall_risk'] = np.clip(self.df['overall_risk'], 0, 1)\n        \n        print(\"Anomaly features engineered successfully\")\n        return self.df\n    \n    def prepare_ml_features(self):\n        \"\"\"\n        FIXED: Prepare feature matrix for ML models with proper handling\n        \"\"\"\n        if 'overall_risk' not in self.df.columns:\n            self.engineer_anomaly_features()\n        \n        # Select features for ML models\n        feature_columns = [\n            'spd', 'acceleration', 'jerk', 'angular_velocity', 'lateral_acceleration',\n            'heading_change_rate', 'curvature', 'overall_risk',\n            'speed_std_3', 'speed_std_5', 'speed_std_10',\n            'accel_std_3', 'accel_std_5', 'accel_std_10',\n            'acceleration_risk', 'jerk_risk', 'lateral_risk', 'speed_risk'\n        ]\n        \n        # Handle missing values and infinite values\n        self.features_df = self.df[feature_columns].copy()\n        \n        # Replace any remaining NaN or infinite values\n        for col in feature_columns:\n            self.features_df[col] = self.features_df[col].fillna(0)\n            self.features_df[col] = self.features_df[col].replace([np.inf, -np.inf], 0)\n        \n        print(f\"Prepared {len(feature_columns)} features for ML models\")\n        print(f\"Feature matrix shape: {self.features_df.shape}\")\n        print(f\"Any NaN values: {self.features_df.isnull().sum().sum()}\")\n        print(f\"Any infinite values: {np.isinf(self.features_df.values).sum()}\")\n        \n        return self.features_df\n    \n    def train_isolation_forest(self, contamination=0.05):\n        \"\"\"\n        Train Isolation Forest for anomaly detection\n        \"\"\"\n        print(\"Training Isolation Forest...\")\n        \n        if self.features_df is None:\n            self.prepare_ml_features()\n        \n        # Scale features\n        X_scaled = self.scaler.fit_transform(self.features_df)\n        \n        # Train Isolation Forest\n        self.isolation_forest = IsolationForest(\n            contamination=contamination,\n            random_state=42,\n            n_estimators=200,\n            max_samples='auto',\n            max_features=0.8,\n            n_jobs=-1\n        )\n        \n        self.isolation_forest.fit(X_scaled)\n        \n        # Predict anomalies\n        anomaly_scores = self.isolation_forest.decision_function(X_scaled)\n        anomaly_labels = self.isolation_forest.predict(X_scaled)\n        \n        self.df['if_anomaly_score'] = anomaly_scores\n        self.df['if_anomaly'] = (anomaly_labels == -1).astype(int)\n        \n        anomaly_rate = self.df['if_anomaly'].mean()\n        print(f\"Isolation Forest trained. Anomaly rate: {anomaly_rate:.3f}\")\n        \n        return self.isolation_forest\n    \n    def train_one_class_svm(self, nu=0.05):\n        \"\"\"\n        Train One-Class SVM for anomaly detection\n        \"\"\"\n        print(\"Training One-Class SVM...\")\n        \n        if self.features_df is None:\n            self.prepare_ml_features()\n        \n        # Scale features\n        X_scaled = self.scaler.transform(self.features_df)\n        \n        # Train One-Class SVM\n        self.one_class_svm = OneClassSVM(\n            nu=nu,\n            kernel='rbf',\n            gamma='scale',\n            cache_size=1000\n        )\n        \n        # Sample for training if dataset is too large\n        if len(X_scaled) > 50000:\n            sample_idx = np.random.choice(len(X_scaled), 50000, replace=False)\n            X_sample = X_scaled[sample_idx]\n        else:\n            X_sample = X_scaled\n        \n        self.one_class_svm.fit(X_sample)\n        \n        # Predict anomalies on full dataset\n        anomaly_scores = self.one_class_svm.decision_function(X_scaled)\n        anomaly_labels = self.one_class_svm.predict(X_scaled)\n        \n        self.df['svm_anomaly_score'] = anomaly_scores\n        self.df['svm_anomaly'] = (anomaly_labels == -1).astype(int)\n        \n        anomaly_rate = self.df['svm_anomaly'].mean()\n        print(f\"One-Class SVM trained. Anomaly rate: {anomaly_rate:.3f}\")\n        \n        return self.one_class_svm\n    \n    def prepare_sequences(self, sequence_length=20):\n        \"\"\"\n        Prepare sequential data for LSTM training\n        \"\"\"\n        if self.features_df is None:\n            self.prepare_ml_features()\n        \n        # Scale features\n        X_scaled = self.scaler.transform(self.features_df)\n        \n        sequences = []\n        trip_ids = []\n        indices = []\n        \n        # Create sequences for each trip\n        for trip_id in self.df['randomized_id'].unique():\n            trip_mask = self.df['randomized_id'] == trip_id\n            trip_data = X_scaled[trip_mask]\n            \n            if len(trip_data) >= sequence_length:\n                for i in range(len(trip_data) - sequence_length + 1):\n                    sequences.append(trip_data[i:i + sequence_length])\n                    trip_ids.append(trip_id)\n                    indices.append(np.where(trip_mask)[0][i + sequence_length - 1])\n        \n        return np.array(sequences), trip_ids, indices\n    \n    def train_lstm_autoencoder(self, sequence_length=20, epochs=50, batch_size=32, lr=0.001):\n        \"\"\"\n        FIXED: Train LSTM Autoencoder with better parameters\n        \"\"\"\n        print(\"Training LSTM Autoencoder with PyTorch...\")\n        \n        # Prepare sequences\n        X_sequences, trip_ids, indices = self.prepare_sequences(sequence_length)\n        \n        if len(X_sequences) == 0:\n            print(\"No sequences could be created. Check sequence_length parameter.\")\n            return None\n        \n        print(f\"Created {len(X_sequences)} sequences of length {sequence_length}\")\n        \n        # Create model\n        input_dim = X_sequences.shape[2]\n        self.lstm_autoencoder = LSTMAutoencoder(\n            input_dim=input_dim,\n            hidden_dim=64,\n            latent_dim=10,\n            num_layers=2,\n            sequence_length=sequence_length\n        ).to(device)\n        \n        # Split data\n        X_train, X_val = train_test_split(X_sequences, test_size=0.2, random_state=42)\n        \n        # Create datasets and dataloaders\n        train_dataset = SequenceDataset(X_train)\n        val_dataset = SequenceDataset(X_val)\n        \n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n        \n        # Optimizer and loss function\n        optimizer = optim.Adam(self.lstm_autoencoder.parameters(), lr=lr, weight_decay=1e-5)\n        criterion = nn.MSELoss()\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, min_lr=1e-6)\n        \n        # Early stopping\n        early_stopping = EarlyStopping(patience=15, restore_best_weights=True)\n        \n        # Training loop\n        train_losses = []\n        val_losses = []\n        \n        print(f\"Training on {device} for {epochs} epochs...\")\n        \n        for epoch in range(epochs):\n            # Training\n            self.lstm_autoencoder.train()\n            train_loss = 0.0\n            \n            for batch in train_loader:\n                sequences = batch.to(device)\n                \n                optimizer.zero_grad()\n                reconstructed = self.lstm_autoencoder(sequences)\n                loss = criterion(reconstructed, sequences)\n                loss.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(self.lstm_autoencoder.parameters(), max_norm=1.0)\n                \n                optimizer.step()\n                train_loss += loss.item()\n            \n            avg_train_loss = train_loss / len(train_loader)\n            train_losses.append(avg_train_loss)\n            \n            # Validation\n            self.lstm_autoencoder.eval()\n            val_loss = 0.0\n            \n            with torch.no_grad():\n                for batch in val_loader:\n                    sequences = batch.to(device)\n                    reconstructed = self.lstm_autoencoder(sequences)\n                    loss = criterion(reconstructed, sequences)\n                    val_loss += loss.item()\n            \n            avg_val_loss = val_loss / len(val_loader)\n            val_losses.append(avg_val_loss)\n            \n            # Learning rate scheduling\n            scheduler.step(avg_val_loss)\n            \n            # Print progress\n            if (epoch + 1) % 5 == 0 or epoch == 0:\n                print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n            \n            # Early stopping\n            if early_stopping(avg_val_loss, self.lstm_autoencoder):\n                print(f\"Early stopping at epoch {epoch + 1}\")\n                break\n        \n        # Calculate reconstruction errors\n        print(\"Calculating reconstruction errors...\")\n        self.lstm_autoencoder.eval()\n        \n        reconstruction_errors = []\n        batch_size_eval = 64\n        \n        with torch.no_grad():\n            for i in range(0, len(X_sequences), batch_size_eval):\n                batch_sequences = torch.FloatTensor(X_sequences[i:i+batch_size_eval]).to(device)\n                reconstructed = self.lstm_autoencoder(batch_sequences)\n                batch_errors = torch.mean((batch_sequences - reconstructed) ** 2, dim=(1, 2)).cpu().numpy()\n                reconstruction_errors.extend(batch_errors)\n        \n        reconstruction_errors = np.array(reconstruction_errors)\n        \n        # Determine anomaly threshold (95th percentile)\n        self.lstm_threshold = np.percentile(reconstruction_errors, 95)\n        \n        # Mark anomalies\n        self.df['lstm_reconstruction_error'] = 0.0\n        self.df['lstm_anomaly'] = 0\n        \n        for i, idx in enumerate(indices):\n            self.df.loc[idx, 'lstm_reconstruction_error'] = reconstruction_errors[i]\n            self.df.loc[idx, 'lstm_anomaly'] = int(reconstruction_errors[i] > self.lstm_threshold)\n        \n        anomaly_rate = self.df['lstm_anomaly'].mean()\n        print(f\"LSTM Autoencoder trained successfully!\")\n        print(f\"Anomaly rate: {anomaly_rate:.3f}\")\n        print(f\"Reconstruction error threshold: {self.lstm_threshold:.6f}\")\n        print(f\"Final train loss: {avg_train_loss:.6f}\")\n        print(f\"Final val loss: {avg_val_loss:.6f}\")\n        \n        return {\n            'train_losses': train_losses,\n            'val_losses': val_losses,\n            'threshold': self.lstm_threshold,\n            'final_train_loss': avg_train_loss,\n            'final_val_loss': avg_val_loss\n        }\n    \n    def ensemble_anomaly_detection(self, weights=None):\n        \"\"\"\n        FIXED: Combine multiple anomaly detection methods\n        \"\"\"\n        print(\"Creating ensemble anomaly detection...\")\n        \n        if weights is None:\n            weights = {\n                'isolation_forest': 0.35,\n                'one_class_svm': 0.30,\n                'lstm': 0.35\n            }\n        \n        # Initialize ensemble score\n        self.df['ensemble_anomaly_score'] = 0.0\n        n_methods = 0\n        \n        # Isolation Forest contribution\n        if 'if_anomaly_score' in self.df.columns:\n            if_min, if_max = self.df['if_anomaly_score'].min(), self.df['if_anomaly_score'].max()\n            if if_max > if_min:  # Avoid division by zero\n                if_scores_norm = (self.df['if_anomaly_score'] - if_min) / (if_max - if_min)\n                self.df['ensemble_anomaly_score'] += weights['isolation_forest'] * (1 - if_scores_norm)\n                n_methods += 1\n                print(f\"‚úì Isolation Forest scores integrated (range: {if_min:.4f} to {if_max:.4f})\")\n        \n        # One-Class SVM contribution\n        if 'svm_anomaly_score' in self.df.columns:\n            svm_min, svm_max = self.df['svm_anomaly_score'].min(), self.df['svm_anomaly_score'].max()\n            if svm_max > svm_min:  # Avoid division by zero\n                svm_scores_norm = (self.df['svm_anomaly_score'] - svm_min) / (svm_max - svm_min)\n                self.df['ensemble_anomaly_score'] += weights['one_class_svm'] * (1 - svm_scores_norm)\n                n_methods += 1\n                print(f\"‚úì SVM scores integrated (range: {svm_min:.4f} to {svm_max:.4f})\")\n        \n        # LSTM contribution\n        if 'lstm_reconstruction_error' in self.df.columns:\n            lstm_max = self.df['lstm_reconstruction_error'].max()\n            if lstm_max > 0:  # Avoid division by zero\n                lstm_scores_norm = self.df['lstm_reconstruction_error'] / lstm_max\n                self.df['ensemble_anomaly_score'] += weights['lstm'] * lstm_scores_norm\n                n_methods += 1\n                print(f\"‚úì LSTM scores integrated (max error: {lstm_max:.6f})\")\n        \n        if n_methods == 0:\n            print(\"‚ö†Ô∏è No trained models found for ensemble!\")\n            return self.df\n        \n        # Normalize ensemble score\n        if n_methods > 0:\n            self.df['ensemble_anomaly_score'] = self.df['ensemble_anomaly_score'] / sum(weights.values())\n        \n        # Determine final anomaly threshold (top 5% as anomalies)\n        anomaly_threshold = np.percentile(self.df['ensemble_anomaly_score'], 95)\n        self.df['ensemble_anomaly'] = (self.df['ensemble_anomaly_score'] > anomaly_threshold).astype(int)\n        \n        anomaly_rate = self.df['ensemble_anomaly'].mean()\n        print(f\"Ensemble anomaly detection complete.\")\n        print(f\"Anomaly threshold: {anomaly_threshold:.4f}\")\n        print(f\"Final anomaly rate: {anomaly_rate:.3f}\")\n        print(f\"Methods used: {n_methods}/3\")\n        \n        return self.df\n    \n    def save_models(self, save_dir='./saved_models/'):\n        \"\"\"\n        Save all trained models and parameters\n        \"\"\"\n        print(\"Saving all trained models...\")\n        \n        # Create save directory\n        os.makedirs(save_dir, exist_ok=True)\n        \n        models_saved = []\n        \n        # Save Isolation Forest\n        if hasattr(self, 'isolation_forest') and self.isolation_forest is not None:\n            joblib.dump(self.isolation_forest, os.path.join(save_dir, 'isolation_forest.pkl'))\n            models_saved.append('Isolation Forest')\n            print(f\"‚úì Isolation Forest saved to {save_dir}isolation_forest.pkl\")\n        \n        # Save One-Class SVM\n        if hasattr(self, 'one_class_svm') and self.one_class_svm is not None:\n            joblib.dump(self.one_class_svm, os.path.join(save_dir, 'one_class_svm.pkl'))\n            models_saved.append('One-Class SVM')\n            print(f\"‚úì One-Class SVM saved to {save_dir}one_class_svm.pkl\")\n        \n        # Save LSTM Autoencoder\n        if hasattr(self, 'lstm_autoencoder') and self.lstm_autoencoder is not None:\n            torch.save({\n                'model_state_dict': self.lstm_autoencoder.state_dict(),\n                'model_config': {\n                    'input_dim': self.lstm_autoencoder.input_dim,\n                    'hidden_dim': self.lstm_autoencoder.hidden_dim,\n                    'latent_dim': self.lstm_autoencoder.latent_dim,\n                    'num_layers': self.lstm_autoencoder.num_layers,\n                    'sequence_length': self.lstm_autoencoder.sequence_length\n                }\n            }, os.path.join(save_dir, 'lstm_autoencoder.pth'))\n            models_saved.append('LSTM Autoencoder')\n            print(f\"‚úì LSTM Autoencoder saved to {save_dir}lstm_autoencoder.pth\")\n        \n        # Save LSTM threshold\n        if hasattr(self, 'lstm_threshold') and self.lstm_threshold is not None:\n            with open(os.path.join(save_dir, 'lstm_threshold.json'), 'w') as f:\n                json.dump({'lstm_threshold': float(self.lstm_threshold)}, f)\n            models_saved.append('LSTM Threshold')\n            print(f\"‚úì LSTM threshold saved to {save_dir}lstm_threshold.json\")\n        \n        # Save Scaler\n        if hasattr(self, 'scaler') and self.scaler is not None:\n            joblib.dump(self.scaler, os.path.join(save_dir, 'scaler.pkl'))\n            models_saved.append('Scaler')\n            print(f\"‚úì Scaler saved to {save_dir}scaler.pkl\")\n        \n        # Save feature names if available\n        if hasattr(self, 'features_df') and self.features_df is not None:\n            feature_names = list(self.features_df.columns)\n            with open(os.path.join(save_dir, 'feature_names.json'), 'w') as f:\n                json.dump({'feature_names': feature_names}, f)\n            models_saved.append('Feature Names')\n            print(f\"‚úì Feature names saved to {save_dir}feature_names.json\")\n        \n        # Save model metadata\n        metadata = {\n            'models_saved': models_saved,\n            'save_timestamp': pd.Timestamp.now().isoformat(),\n            'device_used': str(device),\n            'total_samples': len(self.df) if hasattr(self, 'df') else 0\n        }\n        \n        with open(os.path.join(save_dir, 'model_metadata.json'), 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        print(f\"\\nüéâ Successfully saved {len(models_saved)} models and components:\")\n        for model in models_saved:\n            print(f\"   ‚Ä¢ {model}\")\n        print(f\"üìÅ All models saved in: {save_dir}\")\n        \n        return save_dir\n    \n    def load_models(self, save_dir='./saved_models/'):\n        \"\"\"\n        Load all previously saved models and parameters\n        \"\"\"\n        print(f\"Loading models from {save_dir}...\")\n        \n        if not os.path.exists(save_dir):\n            print(f\"‚ùå Save directory {save_dir} not found!\")\n            return False\n        \n        models_loaded = []\n        \n        # Load Isolation Forest\n        if_path = os.path.join(save_dir, 'isolation_forest.pkl')\n        if os.path.exists(if_path):\n            self.isolation_forest = joblib.load(if_path)\n            models_loaded.append('Isolation Forest')\n            print(f\"‚úì Isolation Forest loaded from {if_path}\")\n        \n        # Load One-Class SVM\n        svm_path = os.path.join(save_dir, 'one_class_svm.pkl')\n        if os.path.exists(svm_path):\n            self.one_class_svm = joblib.load(svm_path)\n            models_loaded.append('One-Class SVM')\n            print(f\"‚úì One-Class SVM loaded from {svm_path}\")\n        \n        # Load LSTM Autoencoder\n        lstm_path = os.path.join(save_dir, 'lstm_autoencoder.pth')\n        if os.path.exists(lstm_path):\n            checkpoint = torch.load(lstm_path, map_location=device)\n            config = checkpoint['model_config']\n            \n            self.lstm_autoencoder = LSTMAutoencoder(\n                input_dim=config['input_dim'],\n                hidden_dim=config['hidden_dim'],\n                latent_dim=config['latent_dim'],\n                num_layers=config['num_layers'],\n                sequence_length=config['sequence_length']\n            ).to(device)\n            \n            self.lstm_autoencoder.load_state_dict(checkpoint['model_state_dict'])\n            self.lstm_autoencoder.eval()\n            models_loaded.append('LSTM Autoencoder')\n            print(f\"‚úì LSTM Autoencoder loaded from {lstm_path}\")\n        \n        # Load LSTM threshold\n        threshold_path = os.path.join(save_dir, 'lstm_threshold.json')\n        if os.path.exists(threshold_path):\n            with open(threshold_path, 'r') as f:\n                threshold_data = json.load(f)\n            self.lstm_threshold = threshold_data['lstm_threshold']\n            models_loaded.append('LSTM Threshold')\n            print(f\"‚úì LSTM threshold loaded from {threshold_path}\")\n        \n        # Load Scaler\n        scaler_path = os.path.join(save_dir, 'scaler.pkl')\n        if os.path.exists(scaler_path):\n            self.scaler = joblib.load(scaler_path)\n            models_loaded.append('Scaler')\n            print(f\"‚úì Scaler loaded from {scaler_path}\")\n        \n        # Load feature names\n        features_path = os.path.join(save_dir, 'feature_names.json')\n        if os.path.exists(features_path):\n            with open(features_path, 'r') as f:\n                feature_data = json.load(f)\n            self.feature_names = feature_data['feature_names']\n            models_loaded.append('Feature Names')\n            print(f\"‚úì Feature names loaded from {features_path}\")\n        \n        # Load metadata\n        metadata_path = os.path.join(save_dir, 'model_metadata.json')\n        if os.path.exists(metadata_path):\n            with open(metadata_path, 'r') as f:\n                self.model_metadata = json.load(f)\n            print(f\"‚úì Model metadata loaded - saved on {self.model_metadata['save_timestamp']}\")\n        \n        if models_loaded:\n            print(f\"\\nüéâ Successfully loaded {len(models_loaded)} models:\")\n            for model in models_loaded:\n                print(f\"   ‚Ä¢ {model}\")\n            print(f\"üìÅ Models loaded from: {save_dir}\")\n            return True\n        else:\n            print(\"‚ùå No models found to load!\")\n            return False\n    \n    def save_lstm_model_only(self, save_path='./lstm_model.pth'):\n        \"\"\"\n        Save only the LSTM Autoencoder model and threshold\n        \"\"\"\n        if not hasattr(self, 'lstm_autoencoder') or self.lstm_autoencoder is None:\n            print(\"‚ùå No LSTM model to save!\")\n            return False\n        \n        save_data = {\n            'model_state_dict': self.lstm_autoencoder.state_dict(),\n            'model_config': {\n                'input_dim': self.lstm_autoencoder.input_dim,\n                'hidden_dim': self.lstm_autoencoder.hidden_dim,\n                'latent_dim': self.lstm_autoencoder.latent_dim,\n                'num_layers': self.lstm_autoencoder.num_layers,\n                'sequence_length': self.lstm_autoencoder.sequence_length\n            },\n            'threshold': self.lstm_threshold if hasattr(self, 'lstm_threshold') else None,\n            'device': str(device)\n        }\n        \n        torch.save(save_data, save_path)\n        print(f\"‚úì LSTM Autoencoder saved to {save_path}\")\n        return True\n    \n    def load_lstm_model_only(self, load_path='./lstm_model.pth'):\n        \"\"\"\n        Load only the LSTM Autoencoder model and threshold\n        \"\"\"\n        if not os.path.exists(load_path):\n            print(f\"‚ùå LSTM model file {load_path} not found!\")\n            return False\n        \n        checkpoint = torch.load(load_path, map_location=device)\n        config = checkpoint['model_config']\n        \n        self.lstm_autoencoder = LSTMAutoencoder(\n            input_dim=config['input_dim'],\n            hidden_dim=config['hidden_dim'],\n            latent_dim=config['latent_dim'],\n            num_layers=config['num_layers'],\n            sequence_length=config['sequence_length']\n        ).to(device)\n        \n        self.lstm_autoencoder.load_state_dict(checkpoint['model_state_dict'])\n        self.lstm_autoencoder.eval()\n        \n        if 'threshold' in checkpoint and checkpoint['threshold'] is not None:\n            self.lstm_threshold = checkpoint['threshold']\n        \n        print(f\"‚úì LSTM Autoencoder loaded from {load_path}\")\n        return True\n    \n    def analyze_anomaly_patterns(self):\n        \"\"\"\n        FIXED: Analyze detected anomalies with proper handling of NaN values\n        \"\"\"\n        print(\"Analyzing anomaly patterns...\")\n        \n        if 'ensemble_anomaly' not in self.df.columns:\n            self.ensemble_anomaly_detection()\n        \n        anomalies = self.df[self.df['ensemble_anomaly'] == 1]\n        normal = self.df[self.df['ensemble_anomaly'] == 0]\n        \n        # Safe calculation of statistics\n        def safe_mean(series):\n            return series.fillna(0).replace([np.inf, -np.inf], 0).mean()\n        \n        def safe_max(series):\n            return series.fillna(0).replace([np.inf, -np.inf], 0).max()\n        \n        def safe_min(series):\n            return series.fillna(0).replace([np.inf, -np.inf], 0).min()\n        \n        analysis = {\n            'total_anomalies': len(anomalies),\n            'anomaly_rate': len(anomalies) / len(self.df),\n            'speed_comparison': {\n                'anomaly_avg_speed': safe_mean(anomalies['spd']),\n                'normal_avg_speed': safe_mean(normal['spd']),\n                'anomaly_max_speed': safe_max(anomalies['spd'])\n            },\n            'acceleration_comparison': {\n                'anomaly_avg_accel': safe_mean(anomalies['acceleration']),\n                'normal_avg_accel': safe_mean(normal['acceleration']),\n                'anomaly_max_accel': safe_max(anomalies['acceleration']),\n                'anomaly_min_accel': safe_min(anomalies['acceleration'])\n            },\n            'behavior_patterns': {\n                'hard_braking_rate': safe_mean(anomalies['hard_braking']),\n                'hard_acceleration_rate': safe_mean(anomalies['hard_acceleration']),\n                'sharp_turn_rate': safe_mean(anomalies['sharp_turn']),\n                'erratic_steering_rate': safe_mean(anomalies['erratic_steering'])\n            },\n            'risk_scores': {\n                'avg_overall_risk': safe_mean(anomalies['overall_risk']),\n                'avg_acceleration_risk': safe_mean(anomalies['acceleration_risk']),\n                'avg_lateral_risk': safe_mean(anomalies['lateral_risk']),\n                'avg_speed_risk': safe_mean(anomalies['speed_risk'])\n            }\n        }\n        \n        return analysis\n    \n    def get_anomaly_report(self):\n        \"\"\"\n        FIXED: Generate comprehensive anomaly detection report\n        \"\"\"\n        analysis = self.analyze_anomaly_patterns()\n        \n        report = f\"\"\"\n        üö® DRIVING ANOMALY DETECTION REPORT (PyTorch - FIXED) üö®\n        \n        üìä DATASET OVERVIEW:\n        ‚Ä¢ Total Records: {len(self.df):,}\n        ‚Ä¢ Unique Trips: {self.df['randomized_id'].nunique():,}\n        ‚Ä¢ Total Anomalies Detected: {analysis['total_anomalies']:,}\n        ‚Ä¢ Anomaly Rate: {analysis['anomaly_rate']:.2%}\n        \n        üèéÔ∏è SPEED ANALYSIS:\n        ‚Ä¢ Normal Driving Avg Speed: {analysis['speed_comparison']['normal_avg_speed']:.1f} km/h\n        ‚Ä¢ Anomalous Driving Avg Speed: {analysis['speed_comparison']['anomaly_avg_speed']:.1f} km/h\n        ‚Ä¢ Maximum Anomalous Speed: {analysis['speed_comparison']['anomaly_max_speed']:.1f} km/h\n        \n        ‚ö° ACCELERATION ANALYSIS:\n        ‚Ä¢ Normal Avg Acceleration: {analysis['acceleration_comparison']['normal_avg_accel']:.2f} m/s¬≤\n        ‚Ä¢ Anomalous Avg Acceleration: {analysis['acceleration_comparison']['anomaly_avg_accel']:.2f} m/s¬≤\n        ‚Ä¢ Max Acceleration (Anomaly): {analysis['acceleration_comparison']['anomaly_max_accel']:.2f} m/s¬≤\n        ‚Ä¢ Min Acceleration (Hard Braking): {analysis['acceleration_comparison']['anomaly_min_accel']:.2f} m/s¬≤\n        \n        üö® DANGEROUS BEHAVIOR PATTERNS:\n        ‚Ä¢ Hard Braking Rate: {analysis['behavior_patterns']['hard_braking_rate']:.1%}\n        ‚Ä¢ Hard Acceleration Rate: {analysis['behavior_patterns']['hard_acceleration_rate']:.1%}\n        ‚Ä¢ Sharp Turn Rate: {analysis['behavior_patterns']['sharp_turn_rate']:.1%}\n        ‚Ä¢ Erratic Steering Rate: {analysis['behavior_patterns']['erratic_steering_rate']:.1%}\n        \n        ‚ö†Ô∏è RISK ASSESSMENT:\n        ‚Ä¢ Overall Risk Score: {analysis['risk_scores']['avg_overall_risk']:.3f}\n        ‚Ä¢ Acceleration Risk: {analysis['risk_scores']['avg_acceleration_risk']:.3f}\n        ‚Ä¢ Lateral Force Risk: {analysis['risk_scores']['avg_lateral_risk']:.3f}\n        ‚Ä¢ Speed Risk: {analysis['risk_scores']['avg_speed_risk']:.3f}\n        \n        üéØ MODEL PERFORMANCE (PyTorch - FIXED):\n        ‚Ä¢ Device Used: {device}\n        ‚Ä¢ Isolation Forest: {'‚úì' if hasattr(self, 'isolation_forest') and self.isolation_forest else '‚úó'}\n        ‚Ä¢ One-Class SVM: {'‚úì' if hasattr(self, 'one_class_svm') and self.one_class_svm else '‚úó'}\n        ‚Ä¢ LSTM Autoencoder (PyTorch): {'‚úì' if hasattr(self, 'lstm_autoencoder') and self.lstm_autoencoder else '‚úó'}\n        ‚Ä¢ LSTM Threshold: {getattr(self, 'lstm_threshold', 'N/A')}\n        ‚Ä¢ Ensemble Method: {'‚úì' if 'ensemble_anomaly' in self.df.columns else '‚úó'}\n        \"\"\"\n        \n        print(report)\n        return analysis\n\ndef run_complete(df, output_dir='./anomaly_analysis_pytorch_fixed/', epochs=30):\n    \"\"\"\n    Run complete anomaly detection pipeline with PyTorch\n    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"üöÄ Starting Driving Anomaly Detection Pipeline\")\n    print(\"=\" * 75)\n    \n    # Initialize detector\n    detector = DrivingAnomalyDetectorPyTorchFixed(df)\n    \n    # Step 1: Feature Engineering\n    print(\"\\n1Ô∏è‚É£ Calculating physics features...\")\n    detector.calculate_driving_physics()\n    \n    print(\"\\n2Ô∏è‚É£ Engineering anomaly features...\")\n    detector.engineer_anomaly_features()\n    \n    # Step 2: Train ML Models\n    print(\"\\n3Ô∏è‚É£ Training Isolation Forest...\")\n    detector.train_isolation_forest(contamination=0.05)\n    \n    print(\"\\n4Ô∏è‚É£ Training One-Class SVM...\")\n    detector.train_one_class_svm(nu=0.05)\n    \n    # Step 3: Train PyTorch LSTM Model with proper epochs\n    print(f\"\\n5Ô∏è‚É£ Training LSTM Autoencoder (PyTorch) for {epochs} epochs...\")\n    lstm_history = detector.train_lstm_autoencoder(sequence_length=15, epochs=500, batch_size=32)\n    \n    # Step 4: Ensemble Method\n    print(\"\\n6Ô∏è‚É£ Creating ensemble anomaly detection...\")\n    detector.ensemble_anomaly_detection()\n    \n    # Step 5: Analysis\n    print(\"\\n7Ô∏è‚É£ Generating comprehensive report...\")\n    analysis = detector.get_anomaly_report()\n    \n    # Step 6: Save Results and Models\n    print(\"\\nüíæ Saving results and models...\")\n    detector.df.to_csv(f'{output_dir}/anomaly_results_fixed.csv', index=False)\n    \n    # Save all trained models\n    model_save_dir = os.path.join(output_dir, 'models')\n    detector.save_models(model_save_dir)\n    \n    print(f\"\\n‚úÖ Analysis complete! Results and models saved in: {output_dir}\")\n    \n    return detector, analysis\n\ndef load_trained_models(df, model_dir='./saved_models/'):\n    \"\"\"\n    Create a new detector instance and load previously trained models\n    \n    Args:\n        df: New dataframe to analyze\n        model_dir: Directory containing saved models\n    \n    Returns:\n        detector: DrivingAnomalyDetectorPyTorchFixed instance with loaded models\n    \"\"\"\n    print(\"üîÑ Creating detector with pre-trained models...\")\n    \n    # Initialize detector with new data\n    detector = DrivingAnomalyDetectorPyTorchFixed(df)\n    \n    # Load pre-trained models\n    success = detector.load_models(model_dir)\n    \n    if success:\n        print(\"‚úÖ Ready to analyze new data with pre-trained models!\")\n        return detector\n    else:\n        print(\"‚ùå Failed to load models. You may need to train new models.\")\n        return detector\n\n# Example usage with better parameters\nif __name__ == \"__main__\":\n    # Generate synthetic driving data\n    print(\"Generating synthetic driving data with anomalies...\")\n    \n    np.random.seed(42)\n    n_trips = 2000\n    data = []\n    \n    for trip_id in range(n_trips):\n        trip_length = np.random.randint(20, 100)\n        base_speed = np.random.uniform(30, 70)\n        base_lat = np.random.uniform(55.7, 55.8)\n        base_lng = np.random.uniform(37.5, 37.7)\n        is_anomalous = np.random.random() < 0.1\n        \n        for i in range(trip_length):\n            if not is_anomalous or np.random.random() > 0.3:\n                speed = base_speed + np.random.normal(0, 5)\n                speed = max(0, speed)\n                lat = base_lat + i * 0.001 + np.random.normal(0, 0.0001)\n                lng = base_lng + i * 0.001 + np.random.normal(0, 0.0001)\n                azm = np.random.uniform(0, 360)\n            else:\n                anomaly_type = np.random.choice(['speeding', 'hard_braking', 'sharp_turn'])\n                if anomaly_type == 'speeding':\n                    speed = np.random.uniform(90, 150)\n                elif anomaly_type == 'hard_braking':\n                    speed = max(0, base_speed - np.random.uniform(30, 50))\n                else:\n                    speed = base_speed\n                    azm = (np.random.uniform(0, 360) + np.random.uniform(45, 90)) % 360\n                \n                lat = base_lat + i * 0.001 + np.random.normal(0, 0.0002)\n                lng = base_lng + i * 0.001 + np.random.normal(0, 0.0002)\n                if 'azm' not in locals():\n                    azm = np.random.uniform(0, 360)\n            \n            data.append({\n                'randomized_id': f'trip_{trip_id}',\n                'lat': lat,\n                'lng': lng,\n                'alt': np.random.uniform(100, 200),\n                'spd': speed,\n                'azm': azm\n            })\n    \n    df = pd.DataFrame(data)\n    print(f\"Generated {len(df)} data points from {n_trips} trips\")\n    \n    # Run FIXED analysis with proper epochs\n    detector, analysis = run_complete(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T14:35:25.647367Z","iopub.execute_input":"2025-09-13T14:35:25.647659Z","iopub.status.idle":"2025-09-13T15:15:49.015030Z","shell.execute_reply.started":"2025-09-13T14:35:25.647636Z","shell.execute_reply":"2025-09-13T15:15:49.014391Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGenerating synthetic driving data with anomalies...\nGenerated 118166 data points from 2000 trips\nüöÄ Starting Driving Anomaly Detection Pipeline\n===========================================================================\n\n1Ô∏è‚É£ Calculating physics features...\nCalculating driving physics features...\nPhysics features calculated successfully\n\n2Ô∏è‚É£ Engineering anomaly features...\nEngineering anomaly detection features...\nAnomaly features engineered successfully\n\n3Ô∏è‚É£ Training Isolation Forest...\nTraining Isolation Forest...\nPrepared 18 features for ML models\nFeature matrix shape: (118166, 18)\nAny NaN values: 0\nAny infinite values: 0\nIsolation Forest trained. Anomaly rate: 0.050\n\n4Ô∏è‚É£ Training One-Class SVM...\nTraining One-Class SVM...\nOne-Class SVM trained. Anomaly rate: 0.050\n\n5Ô∏è‚É£ Training LSTM Autoencoder (PyTorch) for 30 epochs...\nTraining LSTM Autoencoder with PyTorch...\nCreated 90166 sequences of length 15\nTraining on cuda for 500 epochs...\nEpoch [1/500], Train Loss: 12.096553, Val Loss: 10.446937\nEpoch [5/500], Train Loss: 6.435770, Val Loss: 5.392014\nEpoch [10/500], Train Loss: 4.260674, Val Loss: 3.792433\nEpoch [15/500], Train Loss: 3.183023, Val Loss: 2.740180\nEpoch [20/500], Train Loss: 2.455045, Val Loss: 2.353382\nEpoch [25/500], Train Loss: 2.147679, Val Loss: 2.001454\nEpoch [30/500], Train Loss: 1.984288, Val Loss: 1.840504\nEpoch [35/500], Train Loss: 1.851703, Val Loss: 1.719372\nEpoch [40/500], Train Loss: 1.726438, Val Loss: 1.743260\nEpoch [45/500], Train Loss: 1.668725, Val Loss: 1.666554\nEpoch [50/500], Train Loss: 1.611004, Val Loss: 1.534536\nEpoch [55/500], Train Loss: 1.527290, Val Loss: 1.573867\nEpoch [60/500], Train Loss: 1.496347, Val Loss: 1.373430\nEpoch [65/500], Train Loss: 1.449832, Val Loss: 1.407701\nEpoch [70/500], Train Loss: 1.222330, Val Loss: 1.226312\nEpoch [75/500], Train Loss: 1.194368, Val Loss: 1.227701\nEpoch [80/500], Train Loss: 1.197633, Val Loss: 1.259981\nEpoch [85/500], Train Loss: 1.099183, Val Loss: 1.117206\nEpoch [90/500], Train Loss: 1.126540, Val Loss: 1.109662\nEpoch [95/500], Train Loss: 1.110843, Val Loss: 1.093190\nEpoch [100/500], Train Loss: 1.094079, Val Loss: 1.217470\nEpoch [105/500], Train Loss: 1.062899, Val Loss: 1.056489\nEpoch [110/500], Train Loss: 1.054587, Val Loss: 1.109302\nEpoch [115/500], Train Loss: 1.007179, Val Loss: 1.010685\nEpoch [120/500], Train Loss: 0.987648, Val Loss: 1.019412\nEpoch [125/500], Train Loss: 0.982880, Val Loss: 1.014782\nEpoch [130/500], Train Loss: 0.969600, Val Loss: 1.013585\nEpoch [135/500], Train Loss: 0.965046, Val Loss: 0.997302\nEpoch [140/500], Train Loss: 0.953265, Val Loss: 0.981555\nEpoch [145/500], Train Loss: 0.945473, Val Loss: 0.978548\nEpoch [150/500], Train Loss: 0.940021, Val Loss: 0.991373\nEpoch [155/500], Train Loss: 0.931153, Val Loss: 0.980022\nEpoch [160/500], Train Loss: 0.934835, Val Loss: 0.978227\nEpoch [165/500], Train Loss: 0.920387, Val Loss: 0.972181\nEpoch [170/500], Train Loss: 0.930774, Val Loss: 0.976049\nEpoch [175/500], Train Loss: 0.927540, Val Loss: 0.971545\nEpoch [180/500], Train Loss: 0.924577, Val Loss: 0.974749\nEpoch [185/500], Train Loss: 0.923857, Val Loss: 0.971363\nEpoch [190/500], Train Loss: 0.915734, Val Loss: 0.977182\nEpoch [195/500], Train Loss: 0.915409, Val Loss: 0.977183\nEpoch [200/500], Train Loss: 0.916834, Val Loss: 0.971836\nEarly stopping at epoch 200\nCalculating reconstruction errors...\nLSTM Autoencoder trained successfully!\nAnomaly rate: 0.038\nReconstruction error threshold: 2.915369\nFinal train loss: 0.916834\nFinal val loss: 0.971836\n\n6Ô∏è‚É£ Creating ensemble anomaly detection...\nCreating ensemble anomaly detection...\n‚úì Isolation Forest scores integrated (range: -0.2400 to 0.1680)\n‚úì SVM scores integrated (range: -381.6356 to 106.7346)\n‚úì LSTM scores integrated (max error: 355.047272)\nEnsemble anomaly detection complete.\nAnomaly threshold: 0.2051\nFinal anomaly rate: 0.050\nMethods used: 3/3\n\n7Ô∏è‚É£ Generating comprehensive report...\nAnalyzing anomaly patterns...\n\n        üö® DRIVING ANOMALY DETECTION REPORT (PyTorch - FIXED) üö®\n        \n        üìä DATASET OVERVIEW:\n        ‚Ä¢ Total Records: 118,166\n        ‚Ä¢ Unique Trips: 2,000\n        ‚Ä¢ Total Anomalies Detected: 5,909\n        ‚Ä¢ Anomaly Rate: 5.00%\n        \n        üèéÔ∏è SPEED ANALYSIS:\n        ‚Ä¢ Normal Driving Avg Speed: 49.7 km/h\n        ‚Ä¢ Anomalous Driving Avg Speed: 64.8 km/h\n        ‚Ä¢ Maximum Anomalous Speed: 150.0 km/h\n        \n        ‚ö° ACCELERATION ANALYSIS:\n        ‚Ä¢ Normal Avg Acceleration: -0.01 m/s¬≤\n        ‚Ä¢ Anomalous Avg Acceleration: -0.35 m/s¬≤\n        ‚Ä¢ Max Acceleration (Anomaly): 15.00 m/s¬≤\n        ‚Ä¢ Min Acceleration (Hard Braking): -15.00 m/s¬≤\n        \n        üö® DANGEROUS BEHAVIOR PATTERNS:\n        ‚Ä¢ Hard Braking Rate: 51.2%\n        ‚Ä¢ Hard Acceleration Rate: 48.8%\n        ‚Ä¢ Sharp Turn Rate: 90.3%\n        ‚Ä¢ Erratic Steering Rate: 90.5%\n        \n        ‚ö†Ô∏è RISK ASSESSMENT:\n        ‚Ä¢ Overall Risk Score: 0.719\n        ‚Ä¢ Acceleration Risk: 1.000\n        ‚Ä¢ Lateral Force Risk: 0.902\n        ‚Ä¢ Speed Risk: 0.292\n        \n        üéØ MODEL PERFORMANCE (PyTorch - FIXED):\n        ‚Ä¢ Device Used: cuda\n        ‚Ä¢ Isolation Forest: ‚úì\n        ‚Ä¢ One-Class SVM: ‚úì\n        ‚Ä¢ LSTM Autoencoder (PyTorch): ‚úì\n        ‚Ä¢ LSTM Threshold: 2.9153685569763184\n        ‚Ä¢ Ensemble Method: ‚úì\n        \n\nüíæ Saving results and models...\nSaving all trained models...\n‚úì Isolation Forest saved to ./anomaly_analysis_pytorch_fixed/modelsisolation_forest.pkl\n‚úì One-Class SVM saved to ./anomaly_analysis_pytorch_fixed/modelsone_class_svm.pkl\n‚úì LSTM Autoencoder saved to ./anomaly_analysis_pytorch_fixed/modelslstm_autoencoder.pth\n‚úì LSTM threshold saved to ./anomaly_analysis_pytorch_fixed/modelslstm_threshold.json\n‚úì Scaler saved to ./anomaly_analysis_pytorch_fixed/modelsscaler.pkl\n‚úì Feature names saved to ./anomaly_analysis_pytorch_fixed/modelsfeature_names.json\n\nüéâ Successfully saved 6 models and components:\n   ‚Ä¢ Isolation Forest\n   ‚Ä¢ One-Class SVM\n   ‚Ä¢ LSTM Autoencoder\n   ‚Ä¢ LSTM Threshold\n   ‚Ä¢ Scaler\n   ‚Ä¢ Feature Names\nüìÅ All models saved in: ./anomaly_analysis_pytorch_fixed/models\n\n‚úÖ Analysis complete! Results and models saved in: ./anomaly_analysis_pytorch_fixed/\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class RealTimeAnomalyDetectorFinalFixed:\n    \"\"\"  \n    FINAL FIX: Real-time anomaly detection with proper confidence scoring\n    \"\"\"\n    \n    def __init__(self, trained_detector):\n        self.detector = trained_detector\n        self.buffer = []\n        self.buffer_size = 20\n        self.alert_threshold = 0.3  # Adjusted threshold\n        \n        # Pre-calculate normalization parameters from training data\n        self.setup_normalization_parameters()\n        \n    def setup_normalization_parameters(self):\n        \"\"\"Setup normalization parameters from training data\"\"\"\n        if hasattr(self.detector, 'df') and 'if_anomaly_score' in self.detector.df.columns:\n            self.if_min = self.detector.df['if_anomaly_score'].min()\n            self.if_max = self.detector.df['if_anomaly_score'].max()\n        else:\n            self.if_min, self.if_max = -0.5, 0.5\n            \n        if hasattr(self.detector, 'df') and 'svm_anomaly_score' in self.detector.df.columns:\n            self.svm_min = self.detector.df['svm_anomaly_score'].min()\n            self.svm_max = self.detector.df['svm_anomaly_score'].max()\n        else:\n            self.svm_min, self.svm_max = -2.0, 2.0\n            \n        print(f\"Normalization setup - IF range: [{self.if_min:.4f}, {self.if_max:.4f}]\")\n        print(f\"Normalization setup - SVM range: [{self.svm_min:.4f}, {self.svm_max:.4f}]\")\n        print(f\"LSTM threshold: {getattr(self.detector, 'lstm_threshold', 'N/A')}\")\n        \n    def process_real_time_point(self, data_point):\n        \"\"\"\n        FINAL FIXED: Process a single data point in real-time\n        \"\"\"\n        # Add to buffer\n        self.buffer.append(data_point)\n        \n        # Maintain buffer size\n        if len(self.buffer) > self.buffer_size:\n            self.buffer.pop(0)\n        \n        # Need minimum points for analysis\n        if len(self.buffer) < 5:\n            return {\n                'anomaly_detected': False,\n                'confidence': 0.0,\n                'alert_level': 'NORMAL',\n                'buffer_size': len(self.buffer),\n                'message': f'Building buffer... {len(self.buffer)}/{self.buffer_size}'\n            }\n        \n        try:\n            # Calculate features for current window\n            df_buffer = pd.DataFrame(self.buffer)\n            df_buffer['randomized_id'] = 'real_time'\n            \n            # Use the trained detector's feature engineering\n            temp_detector = DrivingAnomalyDetectorPyTorchFixed(df_buffer)\n            temp_detector.scaler = self.detector.scaler  # Use trained scaler\n            temp_detector.calculate_driving_physics()\n            temp_detector.engineer_anomaly_features()\n            \n            # Get the latest point's features\n            features_df = temp_detector.prepare_ml_features()\n            if len(features_df) == 0:\n                return {'anomaly_detected': False, 'confidence': 0.0, 'alert_level': 'NORMAL', 'error': 'No features'}\n            \n            latest_features = features_df.iloc[-1:].values\n            latest_scaled = self.detector.scaler.transform(latest_features)\n            \n            # Get anomaly scores from trained models\n            scores = {}\n            \n            # Isolation Forest\n            if self.detector.isolation_forest:\n                if_score = self.detector.isolation_forest.decision_function(latest_scaled)[0]\n                scores['isolation_forest'] = if_score\n            \n            # One-Class SVM  \n            if self.detector.one_class_svm:\n                svm_score = self.detector.one_class_svm.decision_function(latest_scaled)[0]\n                scores['one_class_svm'] = svm_score\n            \n            # LSTM Autoencoder\n            lstm_score = 0.0\n            if self.detector.lstm_autoencoder and len(self.buffer) >= 15:  # Need at least 15 points\n                try:\n                    # Get last 15 points for sequence\n                    sequence_features = features_df.iloc[-15:].values\n                    sequence_scaled = self.detector.scaler.transform(sequence_features)\n                    sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(device)\n                    \n                    self.detector.lstm_autoencoder.eval()\n                    with torch.no_grad():\n                        reconstructed = self.detector.lstm_autoencoder(sequence_tensor)\n                        reconstruction_error = torch.mean((sequence_tensor - reconstructed) ** 2).item()\n                        scores['lstm'] = reconstruction_error\n                        lstm_score = reconstruction_error\n                except Exception as e:\n                    print(f\"LSTM error: {e}\")\n                    scores['lstm'] = 0.0\n            \n            # Calculate ensemble score with proper normalization\n            ensemble_score = 0.0\n            score_components = []\n            \n            # Isolation Forest contribution (lower = more anomalous)\n            if 'isolation_forest' in scores:\n                if_raw = scores['isolation_forest']\n                if_range = self.if_max - self.if_min\n                if if_range > 0:\n                    if_normalized = (if_raw - self.if_min) / if_range\n                    if_anomaly_score = 1.0 - np.clip(if_normalized, 0, 1)  # Invert: lower IF score = higher anomaly\n                else:\n                    if_anomaly_score = 0.5\n                \n                ensemble_score += 0.35 * if_anomaly_score\n                score_components.append(f\"IF: {if_anomaly_score:.3f}\")\n            \n            # SVM contribution (negative = more anomalous)\n            if 'one_class_svm' in scores:\n                svm_raw = scores['one_class_svm']\n                svm_range = self.svm_max - self.svm_min\n                if svm_range > 0:\n                    svm_normalized = (svm_raw - self.svm_min) / svm_range\n                    svm_anomaly_score = 1.0 - np.clip(svm_normalized, 0, 1)  # Invert: lower SVM score = higher anomaly\n                else:\n                    svm_anomaly_score = 0.5\n                \n                ensemble_score += 0.30 * svm_anomaly_score\n                score_components.append(f\"SVM: {svm_anomaly_score:.3f}\")\n            \n            # LSTM contribution (higher reconstruction error = more anomalous)\n            if 'lstm' in scores and hasattr(self.detector, 'lstm_threshold'):\n                lstm_raw = scores['lstm']\n                lstm_threshold = self.detector.lstm_threshold\n                if lstm_threshold > 0:\n                    lstm_anomaly_score = np.clip(lstm_raw / lstm_threshold, 0, 1)\n                else:\n                    lstm_anomaly_score = 0.0\n                \n                ensemble_score += 0.35 * lstm_anomaly_score\n                score_components.append(f\"LSTM: {lstm_anomaly_score:.3f}\")\n            \n            # Add rule-based component from current driving behavior\n            current_point = temp_detector.df.iloc[-1]\n            rule_based_score = current_point['overall_risk']\n            ensemble_score += 0.1 * rule_based_score  # Small weight for immediate behavior\n            score_components.append(f\"Rule: {rule_based_score:.3f}\")\n            \n            # Normalize ensemble score to 0-1 range\n            ensemble_score = np.clip(ensemble_score, 0, 1)\n            \n            # Determine alert level\n            if ensemble_score > 0.8:\n                alert_level = 'CRITICAL'\n            elif ensemble_score > 0.6:\n                alert_level = 'HIGH'  \n            elif ensemble_score > 0.4:\n                alert_level = 'MEDIUM'\n            elif ensemble_score > 0.2:\n                alert_level = 'LOW'\n            else:\n                alert_level = 'NORMAL'\n            \n            return {\n                'anomaly_detected': ensemble_score > self.alert_threshold,\n                'confidence': ensemble_score,\n                'alert_level': alert_level,\n                'raw_scores': scores,\n                'score_components': score_components,\n                'details': {\n                    'speed': current_point['spd'],\n                    'acceleration': current_point['acceleration'],\n                    'lateral_acceleration': current_point['lateral_acceleration'],\n                    'overall_risk': current_point['overall_risk'],\n                    'risk_factors': {\n                        'hard_braking': bool(current_point['hard_braking']),\n                        'hard_acceleration': bool(current_point['hard_acceleration']),\n                        'sharp_turn': bool(current_point['sharp_turn']),\n                        'erratic_steering': bool(current_point['erratic_steering'])\n                    }\n                },\n                'buffer_info': {\n                    'size': len(self.buffer),\n                    'lstm_ready': len(self.buffer) >= 15\n                }\n            }\n            \n        except Exception as e:\n            print(f\"Error in real-time processing: {e}\")\n            import traceback\n            traceback.print_exc()\n            return {\n                'anomaly_detected': False,\n                'confidence': 0.0,\n                'alert_level': 'ERROR',\n                'error': str(e)\n            }\n\n# Test the final fixed version\ndef test_realtime_final_fix(detector):\n    \"\"\"Test the final fixed real-time detector\"\"\"\n    print(\"\\nüî¥ Testing FINAL Real-Time Anomaly Detection:\")\n    \n    real_time_detector = RealTimeAnomalyDetectorFinalFixed(detector)\n    \n    # More comprehensive test points\n    test_points = [\n        # Normal driving\n        {'lat': 55.75, 'lng': 37.6, 'spd': 45, 'azm': 90, 'alt': 150},\n        {'lat': 55.751, 'lng': 37.601, 'spd': 47, 'azm': 92, 'alt': 152},\n        {'lat': 55.752, 'lng': 37.602, 'spd': 46, 'azm': 94, 'alt': 154},\n        {'lat': 55.753, 'lng': 37.603, 'spd': 48, 'azm': 96, 'alt': 156},\n        {'lat': 55.754, 'lng': 37.604, 'spd': 49, 'azm': 98, 'alt': 158},\n        \n        # Build up buffer with normal points\n        *[{'lat': 55.75 + i*0.001, 'lng': 37.6 + i*0.001, 'spd': 45 + i, 'azm': 90 + i*2, 'alt': 150 + i} \n          for i in range(5, 15)],\n        \n        # Now test anomalous behaviors\n        {'lat': 55.765, 'lng': 37.615, 'spd': 120, 'azm': 120, 'alt': 170},  # Excessive speed\n        {'lat': 55.766, 'lng': 37.616, 'spd': 15, 'azm': 125, 'alt': 172},   # Hard braking\n        {'lat': 55.767, 'lng': 37.617, 'spd': 50, 'azm': 200, 'alt': 174},   # Sharp turn\n        {'lat': 55.768, 'lng': 37.618, 'spd': 55, 'azm': 140, 'alt': 176},   # Recovery\n    ]\n    \n    for i, point in enumerate(test_points):\n        result = real_time_detector.process_real_time_point(point)\n        \n        if i < 15:  # Buffer building phase\n            if i % 5 == 0:  # Show every 5th point during buffer building\n                print(f\"Point {i+1}: {result['alert_level']} (Confidence: {result['confidence']:.3f}) - {result.get('message', '')}\")\n        else:  # Testing phase\n            print(f\"\\nPoint {i+1}: {result['alert_level']} (Confidence: {result['confidence']:.3f})\")\n            if 'score_components' in result:\n                print(f\"  Score breakdown: {', '.join(result['score_components'])}\")\n            if 'raw_scores' in result:\n                print(f\"  Raw scores: {result['raw_scores']}\")\n            if result['anomaly_detected']:\n                print(f\"  ‚ö†Ô∏è ANOMALY DETECTED! Speed: {result['details']['speed']:.1f} km/h\")\n                print(f\"  Risk factors: {result['details']['risk_factors']}\")\n            print(f\"  Buffer: {result['buffer_info']['size']}/20, LSTM ready: {result['buffer_info']['lstm_ready']}\")\n\n# Run the test\nif __name__ == \"__main__\":\n    # Assuming you have the detector from the previous run\n    test_realtime_final_fix(detector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:17:14.632840Z","iopub.execute_input":"2025-09-13T15:17:14.633565Z","iopub.status.idle":"2025-09-13T15:17:16.417253Z","shell.execute_reply.started":"2025-09-13T15:17:14.633533Z","shell.execute_reply":"2025-09-13T15:17:16.416576Z"}},"outputs":[{"name":"stdout","text":"\nüî¥ Testing FINAL Real-Time Anomaly Detection:\nNormalization setup - IF range: [-0.2400, 0.1680]\nNormalization setup - SVM range: [-381.6356, 106.7346]\nLSTM threshold: 2.9153685569763184\nPoint 1: NORMAL (Confidence: 0.000) - Building buffer... 1/20\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (5, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (6, 18)\nAny NaN values: 0\nAny infinite values: 0\nPoint 6: MEDIUM (Confidence: 0.421) - \nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (7, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (8, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (9, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (10, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (11, 18)\nAny NaN values: 0\nAny infinite values: 0\nPoint 11: MEDIUM (Confidence: 0.424) - \nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (12, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (13, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (14, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (15, 18)\nAny NaN values: 0\nAny infinite values: 0\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (16, 18)\nAny NaN values: 0\nAny infinite values: 0\n\nPoint 16: CRITICAL (Confidence: 0.855)\n  Score breakdown: IF: 0.798, SVM: 0.499, LSTM: 1.000, Rule: 0.762\n  Raw scores: {'isolation_forest': -0.1574138648758675, 'one_class_svm': -136.90409569987563, 'lstm': 62.32437515258789}\n  ‚ö†Ô∏è ANOMALY DETECTED! Speed: 120.0 km/h\n  Risk factors: {'hard_braking': False, 'hard_acceleration': True, 'sharp_turn': True, 'erratic_steering': True}\n  Buffer: 16/20, LSTM ready: True\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (17, 18)\nAny NaN values: 0\nAny infinite values: 0\n\nPoint 17: HIGH (Confidence: 0.773)\n  Score breakdown: IF: 0.663, SVM: 0.434, LSTM: 1.000, Rule: 0.613\n  Raw scores: {'isolation_forest': -0.10240883590906336, 'one_class_svm': -105.31248994934253, 'lstm': 4.881302356719971}\n  ‚ö†Ô∏è ANOMALY DETECTED! Speed: 15.0 km/h\n  Risk factors: {'hard_braking': True, 'hard_acceleration': False, 'sharp_turn': True, 'erratic_steering': True}\n  Buffer: 17/20, LSTM ready: True\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (18, 18)\nAny NaN values: 0\nAny infinite values: 0\n\nPoint 18: HIGH (Confidence: 0.691)\n  Score breakdown: IF: 0.575, SVM: 0.260, LSTM: 1.000, Rule: 0.613\n  Raw scores: {'isolation_forest': -0.0667526930543888, 'one_class_svm': -20.19412237705268, 'lstm': 4.714540004730225}\n  ‚ö†Ô∏è ANOMALY DETECTED! Speed: 50.0 km/h\n  Risk factors: {'hard_braking': True, 'hard_acceleration': False, 'sharp_turn': True, 'erratic_steering': True}\n  Buffer: 18/20, LSTM ready: True\nCalculating driving physics features...\nPhysics features calculated successfully\nEngineering anomaly detection features...\nAnomaly features engineered successfully\nPrepared 18 features for ML models\nFeature matrix shape: (19, 18)\nAny NaN values: 0\nAny infinite values: 0\n\nPoint 19: HIGH (Confidence: 0.605)\n  Score breakdown: IF: 0.419, SVM: 0.158, LSTM: 1.000, Rule: 0.613\n  Raw scores: {'isolation_forest': -0.0027759635635155044, 'one_class_svm': 29.487580663090398, 'lstm': 64.57526397705078}\n  ‚ö†Ô∏è ANOMALY DETECTED! Speed: 55.0 km/h\n  Risk factors: {'hard_braking': True, 'hard_acceleration': False, 'sharp_turn': True, 'erratic_steering': True}\n  Buffer: 19/20, LSTM ready: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport logging\nimport numpy as np\nimport pandas as pd\nimport torch\nimport joblib\nfrom datetime import datetime\nfrom collections import deque\nfrom typing import Dict, List, Optional, Any\nimport asyncio\nimport aiofiles\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom scipy.signal import savgol_filter\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n@dataclass\nclass GPSPoint:\n    \"\"\"GPS data point from tracker - matches your dataset structure\"\"\"\n    vehicle_id: str  # This will be our randomized_id\n    lat: float\n    lng: float\n    alt: float\n    spd: float  # speed in km/h\n    azm: float  # azimuth/heading 0-360\n    timestamp: str = None  # Added for real-time tracking\n    \n    @classmethod\n    def from_tracker_data(cls, tracker_data: Dict) -> 'GPSPoint':\n        \"\"\"Convert from real GPS tracker format to our dataset format\"\"\"\n        return cls(\n            vehicle_id=tracker_data.get('vehicle_id', tracker_data.get('device_id')),\n            lat=tracker_data['lat'],\n            lng=tracker_data['lng'],\n            alt=tracker_data.get('alt', tracker_data.get('altitude', 0.0)),\n            spd=tracker_data.get('spd', tracker_data.get('speed', 0.0)),\n            azm=tracker_data.get('azm', tracker_data.get('heading', 0.0)),\n            timestamp=tracker_data.get('timestamp', datetime.now().isoformat())\n        )\n    \n    def to_dataset_format(self) -> Dict:\n        \"\"\"Convert to the format expected by your trained model\"\"\"\n        return {\n            'randomized_id': self.vehicle_id,\n            'lat': self.lat,\n            'lng': self.lng,\n            'alt': self.alt,\n            'spd': self.spd,\n            'azm': self.azm\n        }\n\n@dataclass\nclass AnomalyResult:\n    \"\"\"Anomaly detection result\"\"\"\n    timestamp: str\n    vehicle_id: str\n    anomaly_detected: bool\n    confidence: float\n    alert_level: str\n    raw_scores: Dict[str, float]\n    driving_metrics: Dict[str, float]\n    risk_factors: Dict[str, bool]\n    \n    def to_dict(self) -> Dict:\n        return asdict(self)\n\n# Import the LSTM model from your training code\nclass LSTMAutoencoder(torch.nn.Module):\n    \"\"\"LSTM Autoencoder - same as your training code\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim=64, latent_dim=10, num_layers=2, sequence_length=20):\n        super(LSTMAutoencoder, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        self.num_layers = num_layers\n        self.sequence_length = sequence_length\n        \n        # Encoder\n        self.encoder_lstm = torch.nn.LSTM(\n            input_dim, hidden_dim, num_layers, \n            batch_first=True, dropout=0.2 if num_layers > 1 else 0\n        )\n        self.encoder_fc = torch.nn.Linear(hidden_dim, latent_dim)\n        \n        # Decoder\n        self.decoder_fc = torch.nn.Linear(latent_dim, hidden_dim)\n        self.decoder_lstm = torch.nn.LSTM(\n            hidden_dim, hidden_dim, num_layers, \n            batch_first=True, dropout=0.2 if num_layers > 1 else 0\n        )\n        self.output_projection = torch.nn.Linear(hidden_dim, input_dim)\n        \n        self.dropout = torch.nn.Dropout(0.2)\n        \n    def encode(self, x):\n        lstm_out, (hidden, cell) = self.encoder_lstm(x)\n        encoded = self.encoder_fc(hidden[-1])\n        return encoded\n    \n    def decode(self, encoded):\n        batch_size = encoded.size(0)\n        decoded = self.decoder_fc(encoded)\n        decoded = decoded.unsqueeze(1).repeat(1, self.sequence_length, 1)\n        lstm_out, _ = self.decoder_lstm(decoded)\n        output = self.output_projection(lstm_out)\n        return output\n    \n    def forward(self, x):\n        encoded = self.encode(x)\n        decoded = self.decode(encoded)\n        return decoded\n\nclass ProductionAnomalyDetector:\n    \"\"\"\n    Production-ready driving anomaly detection system\n    Works with your exact dataset format: randomized_id,lat,lng,alt,spd,azm\n    \"\"\"\n    \n    def __init__(self, model_dir: str, config: Dict = None):\n        \"\"\"\n        Initialize with pre-trained models\n        \"\"\"\n        self.model_dir = Path(model_dir)\n        self.config = config or self._default_config()\n        \n        # Model components\n        self.scaler = None\n        self.isolation_forest = None\n        self.one_class_svm = None\n        self.lstm_autoencoder = None\n        self.lstm_threshold = None\n        \n        # Vehicle buffers for real-time processing\n        self.vehicle_buffers = {}  # vehicle_id -> deque of GPS points\n        self.buffer_size = self.config['buffer_size']\n        \n        # Normalization parameters\n        self.if_min = None\n        self.if_max = None\n        self.svm_min = None\n        self.svm_max = None\n        \n        # Load models\n        self._load_models()\n        \n        logger.info(f\"ProductionAnomalyDetector initialized with models from {model_dir}\")\n        logger.info(f\"Using device: {device}\")\n        \n    def _default_config(self) -> Dict:\n        \"\"\"Default configuration matching your training setup\"\"\"\n        return {\n            'buffer_size': 20,\n            'min_points_for_detection': 5,\n            'lstm_sequence_length': 15,\n            'alert_threshold': 0.3,\n            'weights': {\n                'isolation_forest': 0.35,\n                'one_class_svm': 0.30,\n                'lstm': 0.35\n            }\n        }\n    \n    def _load_models(self):\n        \"\"\"Load all pre-trained models\"\"\"\n        try:\n            # Load scaler (required)\n            scaler_path = self.model_dir / 'scaler.pkl'\n            if scaler_path.exists():\n                self.scaler = joblib.load(scaler_path)\n                logger.info(\"‚úì Feature scaler loaded\")\n            else:\n                raise FileNotFoundError(f\"Feature scaler not found: {scaler_path}\")\n            \n            # Load Isolation Forest\n            if_path = self.model_dir / 'isolation_forest.pkl'\n            if if_path.exists():\n                self.isolation_forest = joblib.load(if_path)\n                logger.info(\"‚úì Isolation Forest loaded\")\n            \n            # Load One-Class SVM\n            svm_path = self.model_dir / 'one_class_svm.pkl'\n            if svm_path.exists():\n                self.one_class_svm = joblib.load(svm_path)\n                logger.info(\"‚úì One-Class SVM loaded\")\n            \n            # Load LSTM Autoencoder\n            lstm_path = self.model_dir / 'lstm_autoencoder.pth'\n            if lstm_path.exists():\n                checkpoint = torch.load(lstm_path, map_location=device)\n                lstm_config = checkpoint[\"model_config\"]\n                self.lstm_autoencoder = LSTMAutoencoder(**lstm_config).to(device)\n            \n                self.lstm_autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n                self.lstm_autoencoder.eval()\n                logger.info(\"‚úì LSTM Autoencoder loaded\")\n                self.lstm_threshold = 2.9153685569763184 # fallback threshold\n                logger.info(f\"‚úì LSTM threshold: {self.lstm_threshold}\")\n\n            # Load normalization parameters\n            norm_path = self.model_dir / 'normalization_params.json'\n            if norm_path.exists():\n                with open(norm_path, 'r') as f:\n                    norm_params = json.load(f)\n                    self.if_min = norm_params.get('if_min', -0.2400)\n                    self.if_max = norm_params.get('if_max', 0.1680)\n                    self.svm_min = norm_params.get('svm_min', -381.6356)\n                    self.svm_max = norm_params.get('svm_max', 106.7346)\n                logger.info(\"‚úì Normalization parameters loaded\")\n            else:\n                # Use your actual training values\n                self.if_min, self.if_max = -0.2400, 0.1680\n                self.svm_min, self.svm_max = -381.6356, 106.7346\n                logger.info(\"Using training normalization parameters\")\n            \n            logger.info(\"All models loaded successfully!\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading models: {e}\")\n            raise\n    \n    def process_gps_point(self, gps_point: GPSPoint) -> Optional[AnomalyResult]:\n        \"\"\"\n        Process a single GPS point - main entry point for real-time detection\n        \"\"\"\n        vehicle_id = gps_point.vehicle_id\n        \n        # Initialize vehicle buffer if needed\n        if vehicle_id not in self.vehicle_buffers:\n            self.vehicle_buffers[vehicle_id] = deque(maxlen=self.buffer_size)\n        \n        # Add point to buffer\n        self.vehicle_buffers[vehicle_id].append(gps_point)\n        buffer = self.vehicle_buffers[vehicle_id]\n        \n        # Need minimum points for detection\n        if len(buffer) < self.config['min_points_for_detection']:\n            return None\n        \n        try:\n            # Convert buffer to DataFrame in your exact format\n            buffer_data = []\n            for point in buffer:\n                buffer_data.append(point.to_dataset_format())\n            \n            df_buffer = pd.DataFrame(buffer_data)\n            \n            # Calculate features using your exact feature engineering pipeline\n            features_df = self._calculate_features_exact_pipeline(df_buffer)\n            \n            if len(features_df) == 0:\n                return None\n            \n            # Get latest point features\n            latest_features = features_df.iloc[-1:].values\n            latest_scaled = self.scaler.transform(latest_features)\n            \n            # Get anomaly scores\n            scores = self._get_anomaly_scores(features_df, latest_scaled)\n            \n            # Calculate ensemble score\n            ensemble_score = self._calculate_ensemble_score(scores)\n            \n            # Determine alert level\n            alert_level = self._get_alert_level(ensemble_score)\n            \n            # Extract metrics from the processed features\n            latest_processed = features_df.iloc[-1]\n            driving_metrics = self._extract_driving_metrics_from_features(latest_processed)\n            risk_factors = self._extract_risk_factors_from_features(latest_processed)\n            \n            return AnomalyResult(\n                timestamp=gps_point.timestamp or datetime.now().isoformat(),\n                vehicle_id=vehicle_id,\n                anomaly_detected=ensemble_score > self.config['alert_threshold'],\n                confidence=float(ensemble_score),\n                alert_level=alert_level,\n                raw_scores=scores,\n                driving_metrics=driving_metrics,\n                risk_factors=risk_factors\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error processing GPS point for vehicle {vehicle_id}: {e}\")\n            return None\n    \n    def _calculate_features_exact_pipeline(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate features using EXACT same pipeline as your training code\n        Input: DataFrame with columns [randomized_id, lat, lng, alt, spd, azm]\n        Output: DataFrame with 18 features ready for ML models\n        \"\"\"\n        # Apply the EXACT same feature engineering as your training\n        df_processed = self._apply_physics_calculations(df.copy())\n        df_processed = self._apply_anomaly_feature_engineering(df_processed)\n        features_df = self._prepare_ml_features_exact(df_processed)\n        \n        return features_df\n    \n    def _apply_physics_calculations(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply exact physics calculations from your training code\"\"\"\n        \n        # Sort by trip and create sequence\n        df = df.sort_values(['randomized_id', 'lat', 'lng'])\n        df['sequence'] = df.groupby('randomized_id').cumcount()\n        df['time_delta'] = 1.0  # 1 second intervals\n        \n        def calculate_trip_features(group):\n            if len(group) < 3:\n                # Fill with safe defaults for short trips\n                group['distance'] = 0.0\n                group['speed_smooth'] = group['spd']\n                group['acceleration'] = 0.0\n                group['jerk'] = 0.0\n                group['angular_velocity'] = 0.0\n                group['lateral_acceleration'] = 0.0\n                group['heading_change_rate'] = 0.0\n                group['curvature'] = 0.0\n                return group\n            \n            # Haversine distance calculation\n            def haversine_distance(lat1, lon1, lat2, lon2):\n                R = 6371000  # Earth radius in meters\n                lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n                dlat = lat2 - lat1\n                dlon = lon2 - lon1\n                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n                c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n                return R * c\n            \n            # Calculate distances\n            distances = [0]\n            for i in range(1, len(group)):\n                try:\n                    dist = haversine_distance(\n                        group.iloc[i-1]['lat'], group.iloc[i-1]['lng'],\n                        group.iloc[i]['lat'], group.iloc[i]['lng']\n                    )\n                    dist = min(dist, 1000)  # Cap at 1km to avoid GPS errors\n                    distances.append(dist)\n                except:\n                    distances.append(0)\n            \n            group['distance'] = distances\n            \n            # Smooth speed data\n            if len(group) >= 5:\n                try:\n                    group['speed_smooth'] = savgol_filter(group['spd'], 5, 2)\n                except:\n                    group['speed_smooth'] = group['spd']\n            else:\n                group['speed_smooth'] = group['spd']\n            \n            group['speed_smooth'] = np.maximum(group['speed_smooth'], 0)\n            \n            # Calculate acceleration\n            speed_ms = group['speed_smooth'] / 3.6  # km/h to m/s\n            try:\n                acceleration = np.gradient(speed_ms, group['time_delta'])\n                acceleration = np.clip(acceleration, -15, 15)\n            except:\n                acceleration = np.zeros(len(group))\n            group['acceleration'] = acceleration\n            \n            # Calculate jerk\n            try:\n                jerk = np.gradient(acceleration, group['time_delta'])\n                jerk = np.clip(jerk, -20, 20)\n            except:\n                jerk = np.zeros(len(group))\n            group['jerk'] = jerk\n            \n            # Calculate angular velocity\n            try:\n                azimuth_rad = np.radians(group['azm'])\n                azimuth_unwrapped = np.unwrap(azimuth_rad)\n                angular_velocity = np.gradient(azimuth_unwrapped, group['time_delta'])\n                angular_velocity = np.clip(angular_velocity, -np.pi, np.pi)\n            except:\n                angular_velocity = np.zeros(len(group))\n            group['angular_velocity'] = angular_velocity\n            \n            # Calculate lateral acceleration\n            lateral_acceleration = speed_ms * angular_velocity\n            lateral_acceleration = np.clip(lateral_acceleration, -20, 20)\n            group['lateral_acceleration'] = lateral_acceleration\n            \n            # Calculate heading change rate\n            group['heading_change_rate'] = np.abs(angular_velocity)\n            \n            # Calculate curvature with safe division\n            denominator = speed_ms + 0.1\n            group['curvature'] = np.divide(\n                np.abs(angular_velocity), \n                denominator, \n                out=np.zeros_like(angular_velocity), \n                where=denominator!=0\n            )\n            \n            return group\n        \n        df = df.groupby('randomized_id').apply(calculate_trip_features)\n        df = df.reset_index(drop=True)\n        \n        # Clean any remaining NaN/inf values\n        numeric_columns = ['distance', 'speed_smooth', 'acceleration', 'jerk', \n                          'angular_velocity', 'lateral_acceleration', 'heading_change_rate', 'curvature']\n        \n        for col in numeric_columns:\n            if col in df.columns:\n                df[col] = df[col].fillna(0)\n                df[col] = df[col].replace([np.inf, -np.inf], 0)\n        \n        return df\n    \n    def _apply_anomaly_feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply exact anomaly feature engineering from your training code\"\"\"\n        \n        # Rolling window statistics\n        window_sizes = [3, 5, 10]\n        \n        for window in window_sizes:\n            try:\n                # Speed patterns\n                df[f'speed_std_{window}'] = df.groupby('randomized_id')['spd'].rolling(\n                    window, center=True, min_periods=1).std().reset_index(0, drop=True).fillna(0)\n                df[f'speed_max_{window}'] = df.groupby('randomized_id')['spd'].rolling(\n                    window, center=True, min_periods=1).max().reset_index(0, drop=True).fillna(0)\n                df[f'speed_min_{window}'] = df.groupby('randomized_id')['spd'].rolling(\n                    window, center=True, min_periods=1).min().reset_index(0, drop=True).fillna(0)\n                \n                # Acceleration patterns\n                df[f'accel_std_{window}'] = df.groupby('randomized_id')['acceleration'].rolling(\n                    window, center=True, min_periods=1).std().reset_index(0, drop=True).fillna(0)\n                df[f'accel_max_{window}'] = df.groupby('randomized_id')['acceleration'].rolling(\n                    window, center=True, min_periods=1).max().reset_index(0, drop=True).fillna(0)\n                df[f'accel_min_{window}'] = df.groupby('randomized_id')['acceleration'].rolling(\n                    window, center=True, min_periods=1).min().reset_index(0, drop=True).fillna(0)\n            except:\n                # Fallback values\n                df[f'speed_std_{window}'] = 0\n                df[f'speed_max_{window}'] = df['spd']\n                df[f'speed_min_{window}'] = df['spd']\n                df[f'accel_std_{window}'] = 0\n                df[f'accel_max_{window}'] = df['acceleration']\n                df[f'accel_min_{window}'] = df['acceleration']\n        \n        # Extreme behavior indicators (exact thresholds from training)\n        df['hard_braking'] = (df['acceleration'] < -4.0).astype(int)\n        df['hard_acceleration'] = (df['acceleration'] > 3.0).astype(int)\n        df['excessive_speed'] = (df['spd'] > 80).astype(int)\n        df['sharp_turn'] = (np.abs(df['lateral_acceleration']) > 4.0).astype(int)\n        df['erratic_steering'] = (np.abs(df['heading_change_rate']) > 0.5).astype(int)\n        \n        # Composite risk scores (exact same calculations)\n        df['acceleration_risk'] = np.clip(np.abs(df['acceleration']) / 10.0, 0, 1)\n        df['jerk_risk'] = np.clip(np.abs(df['jerk']) / 5.0, 0, 1)\n        df['lateral_risk'] = np.clip(np.abs(df['lateral_acceleration']) / 8.0, 0, 1)\n        df['speed_risk'] = np.clip(np.maximum(0, (df['spd'] - 60) / 40.0), 0, 1)\n        \n        # Overall risk score (exact same weights)\n        df['overall_risk'] = (\n            df['acceleration_risk'] * 0.25 +\n            df['jerk_risk'] * 0.20 +\n            df['lateral_risk'] * 0.25 +\n            df['speed_risk'] * 0.15 +\n            (df['hard_braking'] + df['hard_acceleration'] + \n             df['sharp_turn'] + df['erratic_steering']) * 0.15 / 4\n        )\n        \n        df['overall_risk'] = np.clip(df['overall_risk'], 0, 1)\n        \n        return df\n    \n    def _prepare_ml_features_exact(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Prepare exact same 18 features as in training\"\"\"\n        \n        # Exact same feature columns as your training\n        feature_columns = [\n            'spd', 'acceleration', 'jerk', 'angular_velocity', 'lateral_acceleration',\n            'heading_change_rate', 'curvature', 'overall_risk',\n            'speed_std_3', 'speed_std_5', 'speed_std_10',\n            'accel_std_3', 'accel_std_5', 'accel_std_10',\n            'acceleration_risk', 'jerk_risk', 'lateral_risk', 'speed_risk'\n        ]\n        \n        features_df = df[feature_columns].copy()\n        \n        # Clean any remaining issues\n        for col in feature_columns:\n            features_df[col] = features_df[col].fillna(0)\n            features_df[col] = features_df[col].replace([np.inf, -np.inf], 0)\n        \n        return features_df\n    \n    def _get_anomaly_scores(self, features_df: pd.DataFrame, latest_scaled: np.ndarray) -> Dict[str, float]:\n        \"\"\"Get anomaly scores from all models\"\"\"\n        scores = {}\n        \n        # Isolation Forest\n        if self.isolation_forest:\n            scores['isolation_forest'] = float(self.isolation_forest.decision_function(latest_scaled)[0])\n        \n        # One-Class SVM\n        if self.one_class_svm:\n            scores['one_class_svm'] = float(self.one_class_svm.decision_function(latest_scaled)[0])\n        \n        # LSTM Autoencoder\n        if self.lstm_autoencoder and len(features_df) >= self.config['lstm_sequence_length']:\n            try:\n                sequence_length = self.config['lstm_sequence_length']\n                sequence_features = features_df.iloc[-sequence_length:].values\n                sequence_scaled = self.scaler.transform(sequence_features)\n                sequence_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(device)\n                \n                with torch.no_grad():\n                    reconstructed = self.lstm_autoencoder(sequence_tensor)\n                    reconstruction_error = torch.mean((sequence_tensor - reconstructed) ** 2).item()\n                    scores['lstm'] = float(reconstruction_error)\n            except Exception as e:\n                logger.warning(f\"LSTM inference error: {e}\")\n                scores['lstm'] = 0.0\n        \n        return scores\n    \n    def _calculate_ensemble_score(self, scores: Dict[str, float]) -> float:\n        \"\"\"Calculate ensemble score using exact same logic as training\"\"\"\n        ensemble_score = 0.0\n        weights = self.config['weights']\n        \n        # Isolation Forest (lower = more anomalous)\n        if 'isolation_forest' in scores:\n            if_range = self.if_max - self.if_min\n            if if_range > 0:\n                if_normalized = (scores['isolation_forest'] - self.if_min) / if_range\n                if_anomaly_score = 1.0 - np.clip(if_normalized, 0, 1)\n            else:\n                if_anomaly_score = 0.5\n            ensemble_score += weights['isolation_forest'] * if_anomaly_score\n        \n        # SVM (negative = more anomalous)\n        if 'one_class_svm' in scores:\n            svm_range = self.svm_max - self.svm_min\n            if svm_range > 0:\n                svm_normalized = (scores['one_class_svm'] - self.svm_min) / svm_range\n                svm_anomaly_score = 1.0 - np.clip(svm_normalized, 0, 1)\n            else:\n                svm_anomaly_score = 0.5\n            ensemble_score += weights['one_class_svm'] * svm_anomaly_score\n        \n        # LSTM (higher reconstruction error = more anomalous)\n        if 'lstm' in scores and self.lstm_threshold:\n            lstm_anomaly_score = np.clip(scores['lstm'] / self.lstm_threshold, 0, 1)\n            ensemble_score += weights['lstm'] * lstm_anomaly_score\n        \n        return np.clip(ensemble_score, 0, 1)\n    \n    def _get_alert_level(self, confidence: float) -> str:\n        \"\"\"Determine alert level\"\"\"\n        if confidence > 0.8:\n            return 'CRITICAL'\n        elif confidence > 0.6:\n            return 'HIGH'\n        elif confidence > 0.4:\n            return 'MEDIUM'\n        elif confidence > 0.2:\n            return 'LOW'\n        else:\n            return 'NORMAL'\n    \n    def _extract_driving_metrics_from_features(self, features_row: pd.Series) -> Dict[str, float]:\n        \"\"\"Extract driving metrics from processed features\"\"\"\n        return {\n            'speed': float(features_row['spd']),\n            'acceleration': float(features_row['acceleration']),\n            'lateral_acceleration': float(features_row['lateral_acceleration']),\n            'jerk': float(features_row['jerk']),\n            'heading_change_rate': float(features_row['heading_change_rate']),\n            'overall_risk': float(features_row['overall_risk'])\n        }\n    \n    def _extract_risk_factors_from_features(self, features_row):\n        \"\"\"\n        Extract boolean risk factors from a row of driving features.\n        \"\"\"\n    \n        return {\n            'hard_braking': bool(features_row['acceleration'] < -2.5),        # sudden deceleration\n            'hard_acceleration': bool(features_row['acceleration'] > 2.5),    # sudden acceleration\n            'excessive_speed': bool(features_row['spd'] > 120),               # overspeeding (km/h)\n            'sharp_turn': bool(abs(features_row['lateral_acceleration']) > 3.0),  # strong lateral g-force\n            'erratic_steering': bool(abs(features_row['angular_velocity']) > 30)  # quick steering angle change\n        }\n\n    def get_vehicle_status(self, vehicle_id: str) -> Dict[str, Any]:\n        \"\"\"Get current status of a vehicle\"\"\"\n        if vehicle_id not in self.vehicle_buffers:\n            return {'vehicle_id': vehicle_id, 'status': 'no_data'}\n        \n        buffer = self.vehicle_buffers[vehicle_id]\n        return {\n            'vehicle_id': vehicle_id,\n            'buffer_size': len(buffer),\n            'last_update': buffer[-1].timestamp if buffer else None,\n            'ready_for_detection': len(buffer) >= self.config['min_points_for_detection']\n        }\n\n# Updated API input model to match your data structure\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass GPSPointRequest(BaseModel):\n    \"\"\"API request model matching your dataset columns\"\"\"\n    vehicle_id: str  # maps to randomized_id\n    lat: float\n    lng: float\n    alt: float = 0.0\n    spd: float  # speed in km/h\n    azm: float  # azimuth/heading 0-360\n    timestamp: Optional[str] = None\n\n# Updated sample input/output for your exact data structure\nsample_input_output = {\n    \"input\": {\n        \"vehicle_id\": \"fleet_001\",\n        \"lat\": 55.7558,\n        \"lng\": 37.6176,\n        \"alt\": 156.0,\n        \"spd\": 45.5,\n        \"azm\": 85.0,\n        \"timestamp\": \"2025-09-13T10:31:18Z\"\n    },\n    \"output\": {\n        \"status\": \"detected\",\n        \"result\": {\n            \"timestamp\": \"2025-09-13T10:31:18Z\",\n            \"vehicle_id\": \"fleet_001\",\n            \"anomaly_detected\": False,\n            \"confidence\": 0.156,\n            \"alert_level\": \"NORMAL\",\n            \"raw_scores\": {\n                \"isolation_forest\": 0.045,\n                \"one_class_svm\": 12.34,\n                \"lstm\": 0.234\n            },\n            \"driving_metrics\": {\n                \"speed\": 45.5,\n                \"acceleration\": 0.12,\n                \"lateral_acceleration\": 0.08,\n                \"jerk\": 0.05,\n                \"heading_change_rate\": 0.02,\n                \"overall_risk\": 0.089\n            },\n            \"risk_factors\": {\n                \"hard_braking\": False,\n                \"hard_acceleration\": False,\n                \"excessive_speed\": False,\n                \"sharp_turn\": False,\n                \"erratic_steering\": False\n            }\n        }\n    }\n}\n\nif __name__ == \"__main__\":\n    print(\"Production Anomaly Detector for AdilzhanB's dataset format:\")\n    print(\"Columns: randomized_id, lat, lng, alt, spd, azm\")\n    print(\"Ready for deployment!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:45:45.005529Z","iopub.execute_input":"2025-09-13T15:45:45.005824Z","iopub.status.idle":"2025-09-13T15:45:45.069850Z","shell.execute_reply.started":"2025-09-13T15:45:45.005797Z","shell.execute_reply":"2025-09-13T15:45:45.069224Z"}},"outputs":[{"name":"stdout","text":"Production Anomaly Detector for AdilzhanB's dataset format:\nColumns: randomized_id, lat, lng, alt, spd, azm\nReady for deployment!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Optional, Tuple, Any\nfrom datetime import datetime, timedelta\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass BatchAnomalyDetector(ProductionAnomalyDetector):\n    \"\"\"\n    Extended ProductionAnomalyDetector with batch processing capabilities\n    Processes data as list of lists: [[id, lat, lng, azm, spd, alt], ...]\n    \"\"\"\n    \n    def __init__(self, model_dir: str, config: Dict = None):\n        super().__init__(model_dir, config)\n        self.batch_results = []\n        \n    def process_batch_list_of_lists(self, \n                                   data: List[List], \n                                   column_order: List[str] = None,\n                                   sort_by_vehicle: bool = True,\n                                   generate_timestamps: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Process batch data as list of lists\n        \n        Args:\n            data: List of lists in format [[id, lat, lng, azm, spd, alt], ...]\n            column_order: Order of columns if different from default\n            sort_by_vehicle: Whether to sort by vehicle_id for proper sequence\n            generate_timestamps: Whether to generate timestamps automatically\n            \n        Returns:\n            Dictionary with batch processing results\n        \"\"\"\n        \n        if column_order is None:\n            column_order = ['vehicle_id', 'lat', 'lng', 'azm', 'spd', 'alt']\n        \n        print(f\"üîÑ Processing batch of {len(data)} GPS points...\")\n        \n        # Convert list of lists to DataFrame\n        df = pd.DataFrame(data, columns=column_order)\n        \n        # Rename to match your training format\n        column_mapping = {\n            'vehicle_id': 'randomized_id',\n            'azm': 'azm',\n            'spd': 'spd', \n            'alt': 'alt',\n            'lat': 'lat',\n            'lng': 'lng'\n        }\n        \n        # Apply column mapping if needed\n        for old_col, new_col in column_mapping.items():\n            if old_col in df.columns and old_col != new_col:\n                df = df.rename(columns={old_col: new_col})\n        \n        # Ensure we have the right columns\n        required_columns = ['randomized_id', 'lat', 'lng', 'alt', 'spd', 'azm']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        \n        if missing_columns:\n            raise ValueError(f\"Missing required columns: {missing_columns}\")\n        \n        # Sort by vehicle and add sequence if requested\n        if sort_by_vehicle:\n            df = df.sort_values(['randomized_id', 'lat', 'lng']).reset_index(drop=True)\n        \n        # Generate timestamps if requested\n        if generate_timestamps:\n            df['timestamp'] = self._generate_timestamps(df)\n        \n        # Process batch\n        return self._process_dataframe_batch(df)\n    \n    def process_batch_by_vehicle(self, \n                                data: List[List],\n                                column_order: List[str] = None,\n                                time_interval_seconds: int = 2) -> Dict[str, List[AnomalyResult]]:\n        \"\"\"\n        Process batch data vehicle by vehicle to maintain proper sequence\n        \n        Args:\n            data: List of lists format\n            column_order: Column order specification\n            time_interval_seconds: Time interval between GPS points\n            \n        Returns:\n            Dictionary with vehicle_id as key and list of results as value\n        \"\"\"\n        \n        if column_order is None:\n            column_order = ['vehicle_id', 'lat', 'lng', 'azm', 'spd', 'alt']\n        \n        # Convert to DataFrame\n        df = pd.DataFrame(data, columns=column_order)\n        \n        # Group by vehicle\n        vehicle_results = {}\n        total_anomalies = 0\n        \n        print(f\"üöõ Processing {df['vehicle_id'].nunique()} vehicles with {len(df)} total points...\")\n        \n        for vehicle_id in df['vehicle_id'].unique():\n            vehicle_data = df[df['vehicle_id'] == vehicle_id].copy()\n            vehicle_data = vehicle_data.sort_values(['lat', 'lng']).reset_index(drop=True)\n            \n            print(f\"\\nüìç Processing vehicle: {vehicle_id} ({len(vehicle_data)} points)\")\n            \n            # Clear vehicle buffer to start fresh\n            if vehicle_id in self.vehicle_buffers:\n                del self.vehicle_buffers[vehicle_id]\n            \n            vehicle_results[vehicle_id] = []\n            vehicle_anomalies = 0\n            \n            # Process points sequentially for this vehicle\n            for idx, row in vehicle_data.iterrows():\n                timestamp = datetime.now() + timedelta(seconds=idx * time_interval_seconds)\n                \n                gps_point = GPSPoint(\n                    vehicle_id=vehicle_id,\n                    lat=row['lat'],\n                    lng=row['lng'],\n                    alt=row.get('alt', 0.0),\n                    spd=row.get('spd', 0.0),\n                    azm=row.get('azm', 0.0),\n                    timestamp=timestamp.isoformat()\n                )\n                \n                result = self.process_gps_point(gps_point)\n                \n                if result:\n                    vehicle_results[vehicle_id].append(result)\n                    if result.anomaly_detected:\n                        vehicle_anomalies += 1\n                        total_anomalies += 1\n                        \n                        # Print anomaly details\n                        print(f\"   üö® Point {idx+1}: {result.alert_level} \"\n                              f\"(Speed: {result.driving_metrics['speed']:.1f} km/h, \"\n                              f\"Conf: {result.confidence:.3f})\")\n                        print(f\"      Risk factors: {result.risk_factors}\")\n            \n            detection_rate = vehicle_anomalies / len(vehicle_results[vehicle_id]) if vehicle_results[vehicle_id] else 0\n            print(f\"   üìä Vehicle summary: {vehicle_anomalies} anomalies out of {len(vehicle_results[vehicle_id])} detections ({detection_rate:.1%})\")\n        \n        print(f\"\\nüéØ Batch Summary:\")\n        print(f\"   Total vehicles: {len(vehicle_results)}\")\n        print(f\"   Total points processed: {len(df)}\")\n        print(f\"   Total anomalies detected: {total_anomalies}\")\n        print(f\"   Overall anomaly rate: {total_anomalies/len(df):.1%}\")\n        \n        return vehicle_results\n    \n    def process_realtime_stream(self, data_stream: List[List], \n                               column_order: List[str] = None,\n                               delay_seconds: float = 2.0,\n                               callback_function = None) -> List[AnomalyResult]:\n        \"\"\"\n        Simulate real-time processing of list-of-lists data\n        \n        Args:\n            data_stream: List of lists to process as real-time stream\n            column_order: Column order\n            delay_seconds: Delay between processing points (simulate real-time)\n            callback_function: Function to call when anomaly is detected\n            \n        Returns:\n            List of all detection results\n        \"\"\"\n        \n        import time\n        \n        if column_order is None:\n            column_order = ['vehicle_id', 'lat', 'lng', 'azm', 'spd', 'alt']\n        \n        print(f\"üî¥ Starting real-time stream simulation with {len(data_stream)} points...\")\n        print(f\"‚è±Ô∏è Processing delay: {delay_seconds} seconds between points\")\n        \n        all_results = []\n        anomaly_count = 0\n        \n        for i, point_data in enumerate(data_stream):\n            # Convert list to GPSPoint\n            point_dict = dict(zip(column_order, point_data))\n            \n            gps_point = GPSPoint(\n                vehicle_id=point_dict['vehicle_id'],\n                lat=point_dict['lat'],\n                lng=point_dict['lng'],\n                alt=point_dict.get('alt', 0.0),\n                spd=point_dict.get('spd', 0.0),\n                azm=point_dict.get('azm', 0.0),\n                timestamp=datetime.now().isoformat()\n            )\n            \n            # Process point\n            result = self.process_gps_point(gps_point)\n            \n            if result:\n                all_results.append(result)\n                \n                # Print status\n                status_icon = \"üü¢\" if result.alert_level == \"NORMAL\" else \"üü°\" if result.alert_level in [\"LOW\", \"MEDIUM\"] else \"üî¥\"\n                print(f\"{status_icon} Point {i+1:3d}: {result.vehicle_id:12s} | \"\n                      f\"{result.alert_level:8s} | Speed: {result.driving_metrics['speed']:5.1f} km/h | \"\n                      f\"Conf: {result.confidence:.3f}\")\n                \n                if result.anomaly_detected:\n                    anomaly_count += 1\n                    print(f\"      üö® ANOMALY DETECTED! {result.risk_factors}\")\n                    \n                    # Call callback function if provided\n                    if callback_function:\n                        callback_function(result, gps_point)\n            else:\n                print(f\"‚è≥ Point {i+1:3d}: {point_dict['vehicle_id']:12s} | Building buffer...\")\n            \n            # Simulate real-time delay\n            if i < len(data_stream) - 1:  # Don't delay after last point\n                time.sleep(delay_seconds)\n        \n        print(f\"\\nüìä Stream Complete:\")\n        print(f\"   Points processed: {len(data_stream)}\")\n        print(f\"   Detections made: {len(all_results)}\")\n        print(f\"   Anomalies found: {anomaly_count}\")\n        print(f\"   Anomaly rate: {anomaly_count/len(all_results)*100:.1f}%\" if all_results else \"   No detections made\")\n        \n        return all_results\n    \n    def _generate_timestamps(self, df: pd.DataFrame) -> List[str]:\n        \"\"\"Generate realistic timestamps for GPS data\"\"\"\n        base_time = datetime.now()\n        timestamps = []\n        \n        for vehicle_id in df['randomized_id'].unique():\n            vehicle_mask = df['randomized_id'] == vehicle_id\n            vehicle_count = vehicle_mask.sum()\n            \n            # Generate timestamps for this vehicle (2-second intervals)\n            for i in range(vehicle_count):\n                timestamp = base_time + timedelta(seconds=i * 2)\n                timestamps.append(timestamp.isoformat())\n        \n        return timestamps\n    \n    def _process_dataframe_batch(self, df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Process DataFrame using the existing feature pipeline\"\"\"\n        \n        # Use your exact feature engineering pipeline\n        features_df = self._calculate_features_exact_pipeline(df)\n        \n        if len(features_df) == 0:\n            return {\n                \"status\": \"error\",\n                \"message\": \"No features could be calculated\",\n                \"processed\": 0,\n                \"anomalies\": 0\n            }\n        \n        # Scale features\n        features_scaled = self.scaler.transform(features_df)\n        \n        # Get anomaly scores for all points\n        anomaly_results = []\n        \n        print(\"üîç Running anomaly detection on all points...\")\n        \n        for i in range(len(features_scaled)):\n            point_scaled = features_scaled[i:i+1]\n            \n            # Get scores from all models\n            scores = {}\n            \n            # Isolation Forest\n            if self.isolation_forest:\n                scores['isolation_forest'] = float(self.isolation_forest.decision_function(point_scaled)[0])\n            \n            # One-Class SVM\n            if self.one_class_svm:\n                scores['one_class_svm'] = float(self.one_class_svm.decision_function(point_scaled)[0])\n            \n            # LSTM (only if we have enough sequence data)\n            if self.lstm_autoencoder and i >= self.config['lstm_sequence_length'] - 1:\n                try:\n                    sequence_start = max(0, i - self.config['lstm_sequence_length'] + 1)\n                    sequence_features = features_scaled[sequence_start:i+1]\n                    \n                    if len(sequence_features) == self.config['lstm_sequence_length']:\n                        sequence_tensor = torch.FloatTensor(sequence_features).unsqueeze(0).to(device)\n                        \n                        with torch.no_grad():\n                            reconstructed = self.lstm_autoencoder(sequence_tensor)\n                            reconstruction_error = torch.mean((sequence_tensor - reconstructed) ** 2).item()\n                            scores['lstm'] = float(reconstruction_error)\n                except:\n                    scores['lstm'] = 0.0\n            \n            # Calculate ensemble score\n            ensemble_score = self._calculate_ensemble_score(scores)\n            alert_level = self._get_alert_level(ensemble_score)\n            \n            # Extract metrics\n            feature_row = features_df.iloc[i]\n            driving_metrics = self._extract_driving_metrics_from_features(feature_row)\n            risk_factors = self._extract_risk_factors_from_features(feature_row)\n            \n            anomaly_results.append({\n                'index': i,\n                'vehicle_id': df.iloc[i]['randomized_id'],\n                'anomaly_detected': ensemble_score > self.config['alert_threshold'],\n                'confidence': ensemble_score,\n                'alert_level': alert_level,\n                'raw_scores': scores,\n                'driving_metrics': driving_metrics,\n                'risk_factors': risk_factors\n            })\n        \n        # Generate summary\n        total_anomalies = sum(1 for r in anomaly_results if r['anomaly_detected'])\n        \n        return {\n            \"status\": \"completed\",\n            \"processed\": len(anomaly_results),\n            \"anomalies\": total_anomalies,\n            \"anomaly_rate\": total_anomalies / len(anomaly_results) if anomaly_results else 0,\n            \"results\": anomaly_results,\n            \"summary\": {\n                \"total_vehicles\": df['randomized_id'].nunique(),\n                \"total_points\": len(df),\n                \"detection_ready_points\": len(anomaly_results),\n                \"anomalies_by_level\": {\n                    level: sum(1 for r in anomaly_results if r['alert_level'] == level)\n                    for level in ['NORMAL', 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL']\n                }\n            }\n        }\n\n# Example usage functions\ndef example_list_of_lists_usage():\n    \"\"\"Example of how to use the batch processor with list of lists\"\"\"\n    \n    print(\"üîÑ Example: Processing List of Lists Data\")\n    print(\"=\" * 50)\n    \n    # Initialize batch detector\n    detector = BatchAnomalyDetector(\"/kaggle/working/anomaly_analysis_pytorch_fixed/models\")\n    \n    # Sample data as list of lists: [vehicle_id, lat, lng, azm, spd, alt]\n    sample_data = [\n        # Normal driving for vehicle_001\n        [\"vehicle_001\", 55.7558, 37.6176, 90.0, 45.0, 156.0],\n        [\"vehicle_001\", 55.7559, 37.6177, 92.0, 47.0, 157.0],\n        [\"vehicle_001\", 55.7560, 37.6178, 94.0, 46.0, 158.0],\n        [\"vehicle_001\", 55.7561, 37.6179, 96.0, 48.0, 159.0],\n        [\"vehicle_001\", 55.7562, 37.6180, 98.0, 49.0, 160.0],\n        \n        # Aggressive driving for vehicle_002\n        [\"vehicle_002\", 55.7600, 37.6200, 180.0, 70.0, 150.0],\n        [\"vehicle_002\", 55.7601, 37.6201, 182.0, 125.0, 151.0],  # Speeding\n        [\"vehicle_002\", 55.7602, 37.6202, 184.0, 15.0, 152.0],   # Hard braking\n        [\"vehicle_002\", 55.7603, 37.6203, 250.0, 55.0, 153.0],   # Sharp turn\n        \n        # Mixed behavior for vehicle_003\n        [\"vehicle_003\", 55.7700, 37.6300, 45.0, 40.0, 145.0],\n        [\"vehicle_003\", 55.7701, 37.6301, 47.0, 42.0, 146.0],\n        [\"vehicle_003\", 55.7702, 37.6302, 49.0, 110.0, 147.0],   # Speed violation\n        [\"vehicle_003\", 55.7703, 37.6303, 51.0, 43.0, 148.0],\n    ]\n    \n    print(f\"Processing {len(sample_data)} GPS points from {len(set(row[0] for row in sample_data))} vehicles...\")\n    \n    # Method 1: Process as batch\n    print(\"\\nüìä Method 1: Batch Processing\")\n    batch_results = detector.process_batch_list_of_lists(sample_data)\n    \n    print(f\"Batch Results:\")\n    print(f\"  Status: {batch_results['status']}\")\n    print(f\"  Points processed: {batch_results['processed']}\")\n    print(f\"  Anomalies detected: {batch_results['anomalies']}\")\n    print(f\"  Anomaly rate: {batch_results['anomaly_rate']:.1%}\")\n    \n    # Method 2: Process by vehicle\n    print(\"\\nüöõ Method 2: Vehicle-by-Vehicle Processing\")\n    vehicle_results = detector.process_batch_by_vehicle(sample_data)\n    \n    for vehicle_id, results in vehicle_results.items():\n        anomaly_count = sum(1 for r in results if r.anomaly_detected)\n        print(f\"  {vehicle_id}: {anomaly_count} anomalies out of {len(results)} detections\")\n    \n    # Method 3: Real-time simulation\n    print(\"\\nüî¥ Method 3: Real-time Stream Simulation (first 8 points)\")\n    \n    def anomaly_callback(result, gps_point):\n        \"\"\"Callback function for when anomaly is detected\"\"\"\n        print(f\"      üìß ALERT SENT: {result.vehicle_id} - {result.alert_level}\")\n    \n    stream_results = detector.process_realtime_stream(\n        sample_data[:8],  # First 8 points\n        delay_seconds=0.5,  # Faster for demo\n        callback_function=anomaly_callback\n    )\n\ndef load_from_csv_example():\n    \"\"\"Example of loading data from CSV and converting to list of lists\"\"\"\n    \n    print(\"\\nüìÅ Example: Loading from CSV\")\n    print(\"=\" * 50)\n    \n    # Simulate CSV loading (you would use pd.read_csv('your_file.csv'))\n    csv_data = \"\"\"vehicle_id,lat,lng,azm,spd,alt\nvehicle_001,55.7558,37.6176,90.0,45.0,156.0\nvehicle_001,55.7559,37.6177,92.0,47.0,157.0\nvehicle_002,55.7600,37.6200,180.0,125.0,150.0\nvehicle_002,55.7601,37.6201,182.0,15.0,151.0\"\"\"\n    \n    # Convert CSV to list of lists\n    from io import StringIO\n    df = pd.read_csv(StringIO(csv_data))\n    \n    # Convert DataFrame to list of lists\n    data_as_lists = df.values.tolist()\n    \n    print(f\"Loaded {len(data_as_lists)} rows from CSV\")\n    print(f\"Column order: {df.columns.tolist()}\")\n    print(f\"Sample data: {data_as_lists[0]}\")\n    \n    # Process with detector\n    detector = BatchAnomalyDetector(\"/kaggle/working/anomaly_analysis_pytorch_fixed/models\")\n    results = detector.process_batch_list_of_lists(\n        data_as_lists, \n        column_order=df.columns.tolist()\n    )\n    \n    print(f\"Processing complete: {results['anomalies']} anomalies detected\")\n\ndef large_dataset_example():\n    \"\"\"Example for processing large datasets efficiently\"\"\"\n    \n    print(\"\\nüî¢ Example: Large Dataset Processing\")\n    print(\"=\" * 50)\n    \n    # Simulate large dataset\n    np.random.seed(42)\n    large_data = []\n    \n    vehicles = [f\"vehicle_{i:03d}\" for i in range(1, 11)]  # 10 vehicles\n    \n    for vehicle in vehicles:\n        for point in range(100):  # 100 points per vehicle\n            lat = 55.7500 + np.random.uniform(-0.01, 0.01)\n            lng = 37.6000 + np.random.uniform(-0.01, 0.01)\n            azm = np.random.uniform(0, 360)\n            spd = np.random.uniform(20, 80) if np.random.random() > 0.1 else np.random.uniform(90, 140)  # 10% aggressive\n            alt = 150 + np.random.uniform(-20, 20)\n            \n            large_data.append([vehicle, lat, lng, azm, spd, alt])\n    \n    print(f\"Generated large dataset: {len(large_data)} points from {len(vehicles)} vehicles\")\n    \n    # Process efficiently\n    detector = BatchAnomalyDetector(\"/kaggle/working/anomaly_analysis_pytorch_fixed/models\")\n    \n    # Process in chunks for memory efficiency\n    chunk_size = 500\n    total_anomalies = 0\n    \n    for i in range(0, len(large_data), chunk_size):\n        chunk = large_data[i:i + chunk_size]\n        print(f\"Processing chunk {i//chunk_size + 1}: points {i+1}-{i+len(chunk)}\")\n        \n        results = detector.process_batch_list_of_lists(chunk)\n        total_anomalies += results['anomalies']\n        \n        print(f\"  Chunk anomalies: {results['anomalies']}\")\n    \n    print(f\"\\nLarge dataset complete:\")\n    print(f\"  Total points: {len(large_data)}\")\n    print(f\"  Total anomalies: {total_anomalies}\")\n    print(f\"  Overall anomaly rate: {total_anomalies/len(large_data):.1%}\")\n\nif __name__ == \"__main__\":\n    print(\"üöÄ Batch Processing Examples for AdilzhanB\")\n    print(\"List of Lists Format: [[id, lat, lng, azm, spd, alt], ...]\")\n    print(\"=\" * 60)\n\n    example_list_of_lists_usage()\n    load_from_csv_example()\n    large_dataset_example()\n    \n    print(\"\\nüéâ All batch processing examples completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T15:45:47.992077Z","iopub.execute_input":"2025-09-13T15:45:47.992355Z","iopub.status.idle":"2025-09-13T15:46:54.591590Z","shell.execute_reply.started":"2025-09-13T15:45:47.992336Z","shell.execute_reply":"2025-09-13T15:46:54.590959Z"}},"outputs":[{"name":"stdout","text":"üöÄ Batch Processing Examples for AdilzhanB\nList of Lists Format: [[id, lat, lng, azm, spd, alt], ...]\n============================================================\nüîÑ Example: Processing List of Lists Data\n==================================================\nProcessing 13 GPS points from 3 vehicles...\n\nüìä Method 1: Batch Processing\nüîÑ Processing batch of 13 GPS points...\nüîç Running anomaly detection on all points...\nBatch Results:\n  Status: completed\n  Points processed: 13\n  Anomalies detected: 9\n  Anomaly rate: 69.2%\n\nüöõ Method 2: Vehicle-by-Vehicle Processing\nüöõ Processing 3 vehicles with 13 total points...\n\nüìç Processing vehicle: vehicle_001 (5 points)\n   üö® Point 5: LOW (Speed: 49.0 km/h, Conf: 0.357)\n      Risk factors: {'hard_braking': False, 'hard_acceleration': True, 'excessive_speed': False, 'sharp_turn': True, 'erratic_steering': False}\n   üìä Vehicle summary: 1 anomalies out of 1 detections (100.0%)\n\nüìç Processing vehicle: vehicle_002 (4 points)\n   üìä Vehicle summary: 0 anomalies out of 0 detections (0.0%)\n\nüìç Processing vehicle: vehicle_003 (4 points)\n   üìä Vehicle summary: 0 anomalies out of 0 detections (0.0%)\n\nüéØ Batch Summary:\n   Total vehicles: 3\n   Total points processed: 13\n   Total anomalies detected: 1\n   Overall anomaly rate: 7.7%\n  vehicle_001: 1 anomalies out of 1 detections\n  vehicle_002: 0 anomalies out of 0 detections\n  vehicle_003: 0 anomalies out of 0 detections\n\nüî¥ Method 3: Real-time Stream Simulation (first 8 points)\nüî¥ Starting real-time stream simulation with 8 points...\n‚è±Ô∏è Processing delay: 0.5 seconds between points\nüü° Point   1: vehicle_001  | LOW      | Speed:  49.0 km/h | Conf: 0.357\n      üö® ANOMALY DETECTED! {'hard_braking': False, 'hard_acceleration': True, 'excessive_speed': False, 'sharp_turn': True, 'erratic_steering': False}\n      üìß ALERT SENT: vehicle_001 - LOW\nüü° Point   2: vehicle_001  | LOW      | Speed:  49.0 km/h | Conf: 0.358\n      üö® ANOMALY DETECTED! {'hard_braking': False, 'hard_acceleration': True, 'excessive_speed': False, 'sharp_turn': True, 'erratic_steering': False}\n      üìß ALERT SENT: vehicle_001 - LOW\nüü¢ Point   3: vehicle_001  | NORMAL   | Speed:  49.0 km/h | Conf: 0.155\nüü¢ Point   4: vehicle_001  | NORMAL   | Speed:  49.0 km/h | Conf: 0.161\nüü° Point   5: vehicle_001  | LOW      | Speed:  49.0 km/h | Conf: 0.291\nüü° Point   6: vehicle_002  | LOW      | Speed:  55.0 km/h | Conf: 0.301\n      üö® ANOMALY DETECTED! {'hard_braking': True, 'hard_acceleration': False, 'excessive_speed': False, 'sharp_turn': True, 'erratic_steering': False}\n      üìß ALERT SENT: vehicle_002 - LOW\nüü° Point   7: vehicle_002  | LOW      | Speed:  55.0 km/h | Conf: 0.308\n      üö® ANOMALY DETECTED! {'hard_braking': True, 'hard_acceleration': False, 'excessive_speed': False, 'sharp_turn': True, 'erratic_steering': False}\n      üìß ALERT SENT: vehicle_002 - LOW\nüü° Point   8: vehicle_002  | LOW      | Speed:  55.0 km/h | Conf: 0.265\n\nüìä Stream Complete:\n   Points processed: 8\n   Detections made: 8\n   Anomalies found: 4\n   Anomaly rate: 50.0%\n\nüìÅ Example: Loading from CSV\n==================================================\nLoaded 4 rows from CSV\nColumn order: ['vehicle_id', 'lat', 'lng', 'azm', 'spd', 'alt']\nSample data: ['vehicle_001', 55.7558, 37.6176, 90.0, 45.0, 156.0]\nüîÑ Processing batch of 4 GPS points...\nüîç Running anomaly detection on all points...\nProcessing complete: 4 anomalies detected\n\nüî¢ Example: Large Dataset Processing\n==================================================\nGenerated large dataset: 1000 points from 10 vehicles\nProcessing chunk 1: points 1-500\nüîÑ Processing batch of 500 GPS points...\nüîç Running anomaly detection on all points...\n  Chunk anomalies: 466\nProcessing chunk 2: points 501-1000\nüîÑ Processing batch of 500 GPS points...\nüîç Running anomaly detection on all points...\n  Chunk anomalies: 449\n\nLarge dataset complete:\n  Total points: 1000\n  Total anomalies: 915\n  Overall anomaly rate: 91.5%\n\nüéâ All batch processing examples completed!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}