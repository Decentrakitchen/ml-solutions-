{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13044767,"sourceType":"datasetVersion","datasetId":8260226}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import silhouette_score\nfrom scipy.spatial.distance import pdist, squareform\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass AdvancedGeoTrackAnalyzer:\n    def __init__(self, data_path_or_df, sample_size=400000):\n        \"\"\"\n        Initialize the analyzer with data path or DataFrame\n        \n        Parameters:\n        data_path_or_df: str or pandas.DataFrame - Path to CSV file or DataFrame\n        sample_size: int - Maximum number of rows to use for training (default 400k)\n        \"\"\"\n        if isinstance(data_path_or_df, str):\n            print(f\"Loading data from {data_path_or_df}\")\n            self.df = pd.read_csv(data_path_or_df)\n        else:\n            self.df = data_path_or_df.copy()\n        \n        print(f\"Original dataset size: {len(self.df):,} rows\")\n        print(f\"Available columns: {list(self.df.columns)}\")\n        \n        # Sample data if it's too large\n        if len(self.df) > sample_size:\n            print(f\"Sampling {sample_size:,} rows from {len(self.df):,} total rows\")\n            self.df = self.df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n            print(f\"Using sampled dataset of {len(self.df):,} rows\")\n        \n        self.processed_df = None\n        self.routes = None\n        self.tight_places = None\n        \n    def preprocess_data(self):\n        \"\"\"Preprocess the geo-tracking data\"\"\"\n        print(\"Preprocessing data...\")\n        \n        # Make a copy for processing\n        self.processed_df = self.df.copy()\n        \n        # Reset index to avoid ambiguity issues\n        self.processed_df = self.processed_df.reset_index(drop=True)\n        \n        # Check for required columns\n        required_cols = ['randomized_id', 'lat', 'lng']\n        missing_cols = [col for col in required_cols if col not in self.processed_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        \n        # Check for optional columns\n        has_speed = 'spd' in self.processed_df.columns\n        has_azimuth = 'azm' in self.processed_df.columns\n        \n        print(f\"Speed data available: {has_speed}\")\n        print(f\"Azimuth data available: {has_azimuth}\")\n        \n        # Sort by randomized_id for trajectory analysis\n        self.processed_df = self.processed_df.sort_values(['randomized_id']).reset_index(drop=True)\n        \n        # Feature engineering\n        print(\"Creating derived features...\")\n        \n        # Group by randomized_id to calculate trajectory features\n        grouped = self.processed_df.groupby('randomized_id')\n        \n        # Calculate distance between consecutive points in each trajectory\n        def haversine_distance(lat1, lon1, lat2, lon2):\n            \"\"\"Calculate the great circle distance between two points on earth\"\"\"\n            # Convert decimal degrees to radians\n            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n            \n            # Haversine formula\n            dlat = lat2 - lat1\n            dlon = lon2 - lon1\n            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n            c = 2 * np.arcsin(np.sqrt(a))\n            r = 6371  # Radius of earth in kilometers\n            return c * r * 1000  # Convert to meters\n        \n        # Calculate distance between consecutive points\n        lat_prev = grouped['lat'].shift(1)\n        lng_prev = grouped['lng'].shift(1)\n        \n        self.processed_df['distance_to_prev'] = haversine_distance(\n            lat_prev, lng_prev, \n            self.processed_df['lat'], self.processed_df['lng']\n        ).fillna(0)\n        \n        # Speed-related features if speed data is available\n        if has_speed:\n            self.processed_df['speed_change'] = grouped['spd'].diff().fillna(0)\n        else:\n            # Estimate speed from distance (assuming 1 second intervals)\n            self.processed_df['estimated_speed'] = self.processed_df['distance_to_prev'] * 3.6  # m/s to km/h\n            self.processed_df['speed_change'] = grouped['estimated_speed'].diff().fillna(0)\n        \n        # Direction features if azimuth data is available\n        if has_azimuth:\n            self.processed_df['direction_change'] = grouped['azm'].diff().fillna(0)\n        else:\n            # Calculate bearing between consecutive points\n            def calculate_bearing(lat1, lon1, lat2, lon2):\n                lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n                dlon = lon2 - lon1\n                y = np.sin(dlon) * np.cos(lat2)\n                x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n                bearing = np.degrees(np.arctan2(y, x))\n                return (bearing + 360) % 360\n            \n            bearing = calculate_bearing(\n                lat_prev, lng_prev,\n                self.processed_df['lat'], self.processed_df['lng']\n            )\n            self.processed_df['calculated_bearing'] = bearing\n            self.processed_df['direction_change'] = grouped['calculated_bearing'].diff().fillna(0)\n        \n        # Remove rows with invalid coordinates\n        self.processed_df = self.processed_df[\n            (self.processed_df['lat'].between(-90, 90)) & \n            (self.processed_df['lng'].between(-180, 180))\n        ].reset_index(drop=True)\n        \n        print(f\"Preprocessing complete. Final dataset: {len(self.processed_df):,} rows\")\n    def identify_popular_routes(self, eps_route=0.01, min_samples_route=5):\n        \"\"\"Identify popular routes by clustering start-end point pairs - Compatible with generate_report\"\"\"\n        print(\"Identifying popular routes...\")\n        \n        if self.processed_df is None:\n            raise ValueError(\"Data must be preprocessed first\")\n        \n        # Extract start and end points for each trajectory\n        print(\"Extracting trajectory start and end points...\")\n        trajectory_summary = self.processed_df.groupby('randomized_id').agg({\n            'lat': ['first', 'last', 'count'],\n            'lng': ['first', 'last']\n        }).reset_index()\n        \n        # Flatten column names\n        trajectory_summary.columns = [\n            'randomized_id', 'start_lat', 'end_lat', 'point_count', 'start_lng', 'end_lng'\n        ]\n        \n        print(f\"Total trajectories: {len(trajectory_summary)}\")\n        \n        # Filter trajectories with minimum points (at least 3 points to be considered a route)\n        valid_trajectories = trajectory_summary[trajectory_summary['point_count'] >= 3].copy()\n        print(f\"Trajectories with ≥3 points: {len(valid_trajectories)}\")\n        \n        if len(valid_trajectories) == 0:\n            print(\"No valid trajectories found\")\n            self.routes = {}\n            return {}\n        \n        # Calculate route distances to filter out very short routes\n        valid_trajectories['route_distance_deg'] = np.sqrt(\n            (valid_trajectories['end_lat'] - valid_trajectories['start_lat'])**2 + \n            (valid_trajectories['end_lng'] - valid_trajectories['start_lng'])**2\n        )\n        \n        # Use a more lenient distance threshold\n        distance_threshold = valid_trajectories['route_distance_deg'].quantile(0.1)  # Bottom 10%\n        print(f\"Distance threshold: {distance_threshold:.6f} degrees\")\n        \n        # Filter out very short routes\n        meaningful_routes = valid_trajectories[\n            valid_trajectories['route_distance_deg'] > distance_threshold\n        ].copy()\n        \n        print(f\"Routes after distance filtering: {len(meaningful_routes)}\")\n        \n        if len(meaningful_routes) < min_samples_route:\n            print(f\"Not enough meaningful routes ({len(meaningful_routes)}) for clustering (need at least {min_samples_route})\")\n            # Lower the minimum samples requirement\n            min_samples_route = max(2, len(meaningful_routes) // 5)\n            print(f\"Adjusting min_samples_route to: {min_samples_route}\")\n        \n        if len(meaningful_routes) < 2:\n            print(\"Not enough routes for any clustering\")\n            self.routes = {}\n            return {}\n        \n        # Create route vectors for clustering\n        route_vectors = meaningful_routes[['start_lat', 'start_lng', 'end_lat', 'end_lng']].values\n        \n        print(f\"Route vectors shape: {route_vectors.shape}\")\n        \n        # Initialize routes dictionary\n        self.routes = {}\n    \n        # Try multiple clustering approaches\n        # Method 1: DBSCAN with geographic coordinates\n        print(\"\\nTrying DBSCAN clustering...\")\n        try:\n            # Scale the coordinates\n            scaler = StandardScaler()\n            scaled_routes = scaler.fit_transform(route_vectors)\n            \n            # Try different eps values\n            eps_values = [0.1, 0.2, 0.5, 1.0, 1.5, 2.0]\n            best_eps = None\n            best_clusters = None\n            max_clusters = 0\n            \n            for eps in eps_values:\n                clustering = DBSCAN(eps=eps, min_samples=min_samples_route)\n                cluster_labels = clustering.fit_predict(scaled_routes)\n                n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n                n_noise = list(cluster_labels).count(-1)\n                \n                print(f\"  eps={eps}: {n_clusters} clusters, {n_noise} noise points\")\n                \n                if n_clusters > max_clusters and n_clusters <= len(meaningful_routes) // 2:\n                    max_clusters = n_clusters\n                    best_eps = eps\n                    best_clusters = cluster_labels\n            \n            if best_clusters is not None and max_clusters > 0:\n                print(f\"Best DBSCAN result: eps={best_eps}, {max_clusters} clusters\")\n                \n                unique_clusters = np.unique(best_clusters[best_clusters != -1])\n                \n                for cluster_id in unique_clusters:\n                    cluster_mask = best_clusters == cluster_id\n                    cluster_routes = route_vectors[cluster_mask]\n                    cluster_trajectory_ids = meaningful_routes.loc[\n                        meaningful_routes.index[cluster_mask], 'randomized_id'\n                    ].values\n                    \n                    # Calculate cluster statistics\n                    avg_start_lat = np.mean(cluster_routes[:, 0])\n                    avg_start_lng = np.mean(cluster_routes[:, 1])\n                    avg_end_lat = np.mean(cluster_routes[:, 2])\n                    avg_end_lng = np.mean(cluster_routes[:, 3])\n                    \n                    # Calculate average route length in METERS (for compatibility with generate_report)\n                    route_length_m = np.mean([\n                        self.haversine_distance_m(route[0], route[1], route[2], route[3])\n                        for route in cluster_routes\n                    ])\n                    \n                    self.routes[f\"dbscan_{cluster_id}\"] = {\n                        'route_count': len(cluster_routes),\n                        'trajectory_ids': cluster_trajectory_ids.tolist(),\n                        'avg_start_point': {'lat': avg_start_lat, 'lng': avg_start_lng},\n                        'avg_end_point': {'lat': avg_end_lat, 'lng': avg_end_lng},\n                        'avg_route_length_m': route_length_m,  # In meters for compatibility\n                        'popularity_score': len(cluster_routes) / len(meaningful_routes) * 100,\n                        'method': 'DBSCAN'\n                    }\n        \n        except Exception as e:\n            print(f\"DBSCAN failed: {e}\")\n        \n        # Method 2: KMeans clustering if DBSCAN didn't work well\n        if len(self.routes) == 0:\n            print(\"\\nTrying KMeans clustering...\")\n            try:\n                # Try different numbers of clusters\n                max_k = min(10, len(meaningful_routes) // 3)\n                \n                if max_k >= 2:\n                    scaler = StandardScaler()\n                    scaled_routes = scaler.fit_transform(route_vectors)\n                    \n                    best_k = 2\n                    best_score = -1\n                    \n                    for k in range(2, max_k + 1):\n                        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n                        cluster_labels = kmeans.fit_predict(scaled_routes)\n                        \n                        # Calculate silhouette score\n                        try:\n                            score = silhouette_score(scaled_routes, cluster_labels)\n                            print(f\"  k={k}: silhouette score = {score:.3f}\")\n                            \n                            if score > best_score:\n                                best_score = score\n                                best_k = k\n                        except:\n                            continue\n                    \n                    # Use best k\n                    print(f\"Using k={best_k} (best silhouette score: {best_score:.3f})\")\n                    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n                    cluster_labels = kmeans.fit_predict(scaled_routes)\n                    \n                    for cluster_id in range(best_k):\n                        cluster_mask = cluster_labels == cluster_id\n                        cluster_routes = route_vectors[cluster_mask]\n                        cluster_trajectory_ids = meaningful_routes.loc[\n                            meaningful_routes.index[cluster_mask], 'randomized_id'\n                        ].values\n                        \n                        if len(cluster_routes) >= 2:  # At least 2 routes in cluster\n                            # Calculate cluster statistics\n                            avg_start_lat = np.mean(cluster_routes[:, 0])\n                            avg_start_lng = np.mean(cluster_routes[:, 1])\n                            avg_end_lat = np.mean(cluster_routes[:, 2])\n                            avg_end_lng = np.mean(cluster_routes[:, 3])\n                            \n                            # Calculate average route length in METERS\n                            route_length_m = np.mean([\n                                self.haversine_distance_m(route[0], route[1], route[2], route[3])\n                                for route in cluster_routes\n                            ])\n                            \n                            self.routes[f\"kmeans_{cluster_id}\"] = {\n                                'route_count': len(cluster_routes),\n                                'trajectory_ids': cluster_trajectory_ids.tolist(),\n                                'avg_start_point': {'lat': avg_start_lat, 'lng': avg_start_lng},\n                                'avg_end_point': {'lat': avg_end_lat, 'lng': avg_end_lng},\n                                'avg_route_length_m': route_length_m,  # In meters for compatibility\n                                'popularity_score': len(cluster_routes) / len(meaningful_routes) * 100,\n                                'method': 'KMeans'\n                            }\n            \n            except Exception as e:\n                print(f\"KMeans failed: {e}\")\n        \n        # Method 3: Simple grid-based clustering if both fail\n        if len(self.routes) == 0:\n            print(\"\\nTrying grid-based clustering...\")\n            try:\n                # Create a simple grid-based approach\n                lat_bins = 20\n                lng_bins = 20\n                \n                # Create bins for start and end points\n                start_lat_bins = pd.cut(meaningful_routes['start_lat'], bins=lat_bins, labels=False)\n                start_lng_bins = pd.cut(meaningful_routes['start_lng'], bins=lng_bins, labels=False)\n                end_lat_bins = pd.cut(meaningful_routes['end_lat'], bins=lat_bins, labels=False)\n                end_lng_bins = pd.cut(meaningful_routes['end_lng'], bins=lng_bins, labels=False)\n                \n                # Create route signatures\n                meaningful_routes['route_signature'] = (\n                    start_lat_bins.astype(str) + '_' + start_lng_bins.astype(str) + '_' +\n                    end_lat_bins.astype(str) + '_' + end_lng_bins.astype(str)\n                )\n                \n                # Count routes by signature\n                signature_counts = meaningful_routes['route_signature'].value_counts()\n                popular_signatures = signature_counts[signature_counts >= 2]  # At least 2 routes\n                \n                print(f\"Found {len(popular_signatures)} popular route patterns\")\n                \n                for i, (signature, count) in enumerate(popular_signatures.head(10).items()):\n                    cluster_routes_df = meaningful_routes[meaningful_routes['route_signature'] == signature]\n                    \n                    # Calculate average route length in METERS\n                    route_length_m = np.mean([\n                        self.haversine_distance_m(row['start_lat'], row['start_lng'], \n                                                 row['end_lat'], row['end_lng'])\n                        for _, row in cluster_routes_df.iterrows()\n                    ])\n                    \n                    self.routes[f\"grid_{i}\"] = {\n                        'route_count': count,\n                        'trajectory_ids': cluster_routes_df['randomized_id'].tolist(),\n                        'avg_start_point': {\n                            'lat': cluster_routes_df['start_lat'].mean(),\n                            'lng': cluster_routes_df['start_lng'].mean()\n                        },\n                        'avg_end_point': {\n                            'lat': cluster_routes_df['end_lat'].mean(),\n                            'lng': cluster_routes_df['end_lng'].mean()\n                        },\n                        'avg_route_length_m': route_length_m,  # In meters for compatibility\n                        'popularity_score': count / len(meaningful_routes) * 100,\n                        'method': 'Grid-based'\n                    }\n            \n            except Exception as e:\n                print(f\"Grid-based clustering failed: {e}\")\n        \n        # Sort routes by popularity\n        if self.routes:\n            self.routes = dict(sorted(\n                self.routes.items(), \n                key=lambda x: x[1]['route_count'], \n                reverse=True\n            ))\n            \n            print(f\"\\nSuccessfully identified {len(self.routes)} popular route clusters!\")\n            for route_id, route_info in list(self.routes.items())[:5]:\n                print(f\"  {route_id}: {route_info['route_count']} trips ({route_info['popularity_score']:.1f}%)\")\n        else:\n            print(\"No popular routes could be identified\")\n            self.routes = {}\n        \n        return self.routes\n    \n    def haversine_distance_m(self, lat1, lon1, lat2, lon2):\n        \"\"\"Calculate haversine distance in METERS (for compatibility with generate_report)\"\"\"\n        # Convert decimal degrees to radians\n        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n        \n        # Haversine formula\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n        c = 2 * np.arcsin(np.sqrt(a))\n        r = 6371  # Radius of earth in kilometers\n        return c * r * 1000  # Return in METERS                \n    def identify_tight_places(self, eps_tight=0.0005, min_samples_tight=50, density_threshold=0.8):\n        \"\"\"Identify tight places (congestion areas) based on point density and movement patterns\"\"\"\n        print(\"Identifying tight places (congestion areas)...\")\n        \n        if self.processed_df is None:\n            raise ValueError(\"Data must be preprocessed first\")\n        \n        # Use all GPS points for density analysis\n        coords = self.processed_df[['lat', 'lng']].values\n        \n        # Apply DBSCAN clustering to find high-density areas\n        clustering = DBSCAN(eps=eps_tight, min_samples=min_samples_tight)\n        clusters = clustering.fit_predict(coords)\n        \n        # Add cluster labels to dataframe\n        self.processed_df['density_cluster'] = clusters\n        \n        # Analyze each cluster to identify tight places\n        unique_clusters = np.unique(clusters[clusters != -1])\n        \n        self.tight_places = {}\n        for cluster_id in unique_clusters:\n            cluster_mask = clusters == cluster_id\n            cluster_points = coords[cluster_mask]\n            cluster_data = self.processed_df[self.processed_df['density_cluster'] == cluster_id]\n            \n            # Calculate density metrics\n            cluster_area_km2 = self.calculate_cluster_area(cluster_points)\n            point_density = len(cluster_points) / max(cluster_area_km2, 0.001)  # points per km²\n            \n            # Calculate movement characteristics\n            if 'spd' in cluster_data.columns:\n                avg_speed = cluster_data['spd'].mean()\n                speed_variance = cluster_data['spd'].var()\n            else:\n                avg_speed = cluster_data['estimated_speed'].mean()\n                speed_variance = cluster_data['estimated_speed'].var()\n            \n            # Calculate how many unique vehicles pass through this area\n            unique_vehicles = cluster_data['randomized_id'].nunique()\n            \n            # Calculate congestion indicators\n            # Low speed + high density + many vehicles = congestion\n            congestion_score = (point_density * unique_vehicles) / max(avg_speed, 1)\n            \n            # Identify as tight place if meets criteria\n            is_tight_place = (\n                point_density > density_threshold * np.mean([\n                    len(coords[clusters == c]) / max(self.calculate_cluster_area(coords[clusters == c]), 0.001)\n                    for c in unique_clusters\n                ]) and\n                avg_speed < np.percentile(self.processed_df.get('spd', self.processed_df.get('estimated_speed', [30])), 25)\n            )\n            \n            self.tight_places[cluster_id] = {\n                'center_lat': np.mean(cluster_points[:, 0]),\n                'center_lng': np.mean(cluster_points[:, 1]),\n                'point_count': len(cluster_points),\n                'unique_vehicles': unique_vehicles,\n                'area_km2': cluster_area_km2,\n                'point_density_per_km2': point_density,\n                'avg_speed_kmh': avg_speed,\n                'speed_variance': speed_variance,\n                'congestion_score': congestion_score,\n                'is_tight_place': is_tight_place,\n                'severity': 'High' if congestion_score > np.percentile([\n                    (len(coords[clusters == c]) * self.processed_df[self.processed_df['density_cluster'] == c]['randomized_id'].nunique()) / \n                    max(self.processed_df[self.processed_df['density_cluster'] == c].get('spd', self.processed_df[self.processed_df['density_cluster'] == c].get('estimated_speed', [30])).mean(), 1)\n                    for c in unique_clusters\n                ], 75) else 'Medium' if congestion_score > np.percentile([\n                    (len(coords[clusters == c]) * self.processed_df[self.processed_df['density_cluster'] == c]['randomized_id'].nunique()) / \n                    max(self.processed_df[self.processed_df['density_cluster'] == c].get('spd', self.processed_df[self.processed_df['density_cluster'] == c].get('estimated_speed', [30])).mean(), 1)\n                    for c in unique_clusters\n                ], 50) else 'Low'\n            }\n        \n        # Filter to only tight places\n        self.tight_places = {\n            k: v for k, v in self.tight_places.items() \n            if v['is_tight_place']\n        }\n        \n        # Sort by congestion score\n        self.tight_places = dict(sorted(\n            self.tight_places.items(), \n            key=lambda x: x[1]['congestion_score'], \n            reverse=True\n        ))\n        \n        print(f\"Identified {len(self.tight_places)} tight places (congestion areas)\")\n        return self.tight_places\n    \n    def calculate_cluster_area(self, points):\n        \"\"\"Calculate the approximate area of a cluster in km²\"\"\"\n        if len(points) < 3:\n            return 0.001  # Minimum area for small clusters\n        \n        # Use convex hull approach for area calculation\n        from scipy.spatial import ConvexHull\n        \n        try:\n            hull = ConvexHull(points)\n            # Convert to meters using rough approximation\n            lat_to_m = 111000  # meters per degree latitude\n            lng_to_m = 111000 * np.cos(np.radians(np.mean(points[:, 0])))  # adjust for longitude\n            \n            # Scale points to meters\n            points_m = points.copy()\n            points_m[:, 0] *= lat_to_m\n            points_m[:, 1] *= lng_to_m\n            \n            hull_m = ConvexHull(points_m)\n            area_m2 = hull_m.volume  # In 2D, volume gives area\n            area_km2 = area_m2 / 1_000_000  # Convert to km²\n            \n            return max(area_km2, 0.001)  # Minimum area\n        except:\n            # Fallback: bounding box area\n            lat_range = np.max(points[:, 0]) - np.min(points[:, 0])\n            lng_range = np.max(points[:, 1]) - np.min(points[:, 1])\n            area_deg2 = lat_range * lng_range\n            area_km2 = area_deg2 * 111 * 111  # rough conversion\n            return max(area_km2, 0.001)\n    \n    def analyze_route_efficiency(self):\n        \"\"\"Analyze route efficiency and suggest optimizations\"\"\"\n        print(\"Analyzing route efficiency...\")\n        \n        if not self.routes:\n            print(\"No routes identified. Run identify_popular_routes() first.\")\n            return {}\n        \n        efficiency_analysis = {}\n        \n        for route_id, route_info in self.routes.items():\n            trajectory_ids = route_info['trajectory_ids']\n            \n            # Get all trajectories for this route\n            route_trajectories = self.processed_df[\n                self.processed_df['randomized_id'].isin(trajectory_ids)\n            ]\n            \n            # Calculate efficiency metrics\n            total_distances = []\n            total_times = []\n            avg_speeds = []\n            \n            for traj_id in trajectory_ids:\n                traj_data = route_trajectories[route_trajectories['randomized_id'] == traj_id]\n                \n                if len(traj_data) > 1:\n                    total_distance = traj_data['distance_to_prev'].sum()\n                    total_distances.append(total_distance)\n                    \n                    if 'spd' in traj_data.columns:\n                        avg_speed = traj_data['spd'].mean()\n                    else:\n                        avg_speed = traj_data['estimated_speed'].mean()\n                    avg_speeds.append(avg_speed)\n            \n            if total_distances and avg_speeds:\n                efficiency_analysis[route_id] = {\n                    'avg_distance_m': np.mean(total_distances),\n                    'distance_variance': np.var(total_distances),\n                    'avg_speed_kmh': np.mean(avg_speeds),\n                    'speed_consistency': 1 / (1 + np.var(avg_speeds)),  # Higher is more consistent\n                    'efficiency_score': np.mean(avg_speeds) / max(np.mean(total_distances) / 1000, 0.1),  # Speed per km\n                    'route_optimization_potential': 'High' if np.var(total_distances) > np.mean(total_distances) * 0.3 else 'Low'\n                }\n        \n        return efficiency_analysis\n    \n    def create_visualizations(self, output_dir='./geo_analysis_output'):\n        \"\"\"Create comprehensive visualizations\"\"\"\n        import os\n        os.makedirs(output_dir, exist_ok=True)\n        \n        print(\"Creating visualizations...\")\n        \n        # Set up the plotting style\n        plt.style.use('default')\n        sns.set_palette(\"husl\")\n        \n        # 1. Popular Routes Visualization\n        if self.routes:\n            plt.figure(figsize=(15, 10))\n            \n            # Plot all points in light gray\n            plt.scatter(self.processed_df['lng'], self.processed_df['lat'], \n                       c='lightgray', alpha=0.1, s=0.5, label='All GPS Points')\n            \n            # Plot popular routes\n            colors = plt.cm.Set1(np.linspace(0, 1, len(self.routes)))\n            \n            for i, (route_id, route_info) in enumerate(list(self.routes.items())[:10]):  # Top 10 routes\n                start_point = route_info['avg_start_point']\n                end_point = route_info['avg_end_point']\n                \n                # Plot start and end points\n                plt.scatter(start_point['lng'], start_point['lat'], \n                           c=[colors[i]], s=100, marker='o', \n                           label=f'Route {route_id} Start ({route_info[\"route_count\"]} trips)')\n                plt.scatter(end_point['lng'], end_point['lat'], \n                           c=[colors[i]], s=100, marker='s')\n                \n                # Draw line between start and end\n                plt.plot([start_point['lng'], end_point['lng']], \n                        [start_point['lat'], end_point['lat']], \n                        c=colors[i], linewidth=2, alpha=0.7)\n            \n            plt.xlabel('Longitude')\n            plt.ylabel('Latitude')\n            plt.title('Popular Routes Identification\\n(Circle=Start, Square=End)')\n            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(f'{output_dir}/popular_routes.png', dpi=300, bbox_inches='tight')\n            plt.close()\n        \n        # 2. Tight Places (Congestion Areas) Visualization\n        if self.tight_places:\n            plt.figure(figsize=(15, 10))\n            \n            # Plot all points\n            plt.scatter(self.processed_df['lng'], self.processed_df['lat'], \n                       c='lightblue', alpha=0.1, s=0.5, label='All GPS Points')\n            \n            # Plot tight places with size based on congestion score\n            for place_id, place_info in self.tight_places.items():\n                size = min(place_info['congestion_score'] * 10, 500)\n                color = {'High': 'red', 'Medium': 'orange', 'Low': 'yellow'}[place_info['severity']]\n                \n                plt.scatter(place_info['center_lng'], place_info['center_lat'], \n                           s=size, c=color, alpha=0.7, edgecolors='black',\n                           label=f'{place_info[\"severity\"]} Congestion ({place_info[\"unique_vehicles\"]} vehicles)')\n            \n            plt.xlabel('Longitude')\n            plt.ylabel('Latitude')\n            plt.title('Tight Places (Congestion Areas) Identification\\n(Size = Congestion Score)')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.savefig(f'{output_dir}/tight_places.png', dpi=300, bbox_inches='tight')\n            plt.close()\n        \n        # 3. Combined Analysis Map\n        plt.figure(figsize=(15, 10))\n        \n        # Base map\n        plt.scatter(self.processed_df['lng'], self.processed_df['lat'], \n                   c='lightgray', alpha=0.05, s=0.3)\n        \n        # Popular routes\n        if self.routes:\n            route_colors = plt.cm.Blues(np.linspace(0.4, 1, len(self.routes)))\n            for i, (route_id, route_info) in enumerate(list(self.routes.items())[:5]):\n                start_point = route_info['avg_start_point']\n                end_point = route_info['avg_end_point']\n                plt.plot([start_point['lng'], end_point['lng']], \n                        [start_point['lat'], end_point['lat']], \n                        c=route_colors[i], linewidth=3, alpha=0.8,\n                        label=f'Popular Route {route_id}')\n        \n        # Tight places\n        if self.tight_places:\n            for place_id, place_info in self.tight_places.items():\n                size = min(place_info['congestion_score'] * 15, 300)\n                plt.scatter(place_info['center_lng'], place_info['center_lat'], \n                           s=size, c='red', alpha=0.8, marker='X', edgecolors='darkred',\n                           label='Congestion Area' if place_id == list(self.tight_places.keys())[0] else \"\")\n        \n        plt.xlabel('Longitude')\n        plt.ylabel('Latitude')\n        plt.title('Combined Analysis: Popular Routes & Congestion Areas')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(f'{output_dir}/combined_analysis.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # 4. Statistics Dashboard\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Route popularity distribution\n        if self.routes:\n            route_counts = [info['route_count'] for info in self.routes.values()]\n            axes[0, 0].bar(range(len(route_counts)), route_counts, color='skyblue')\n            axes[0, 0].set_xlabel('Route Cluster ID')\n            axes[0, 0].set_ylabel('Number of Trips')\n            axes[0, 0].set_title('Route Popularity Distribution')\n            axes[0, 0].grid(True, alpha=0.3)\n        \n        # Congestion severity distribution\n        if self.tight_places:\n            severity_counts = {}\n            for place_info in self.tight_places.values():\n                severity = place_info['severity']\n                severity_counts[severity] = severity_counts.get(severity, 0) + 1\n            \n            axes[0, 1].pie(severity_counts.values(), labels=severity_counts.keys(), \n                          autopct='%1.1f%%', colors=['red', 'orange', 'yellow'])\n            axes[0, 1].set_title('Congestion Severity Distribution')\n        \n        # Speed distribution\n        speed_col = 'spd' if 'spd' in self.processed_df.columns else 'estimated_speed'\n        if speed_col in self.processed_df.columns:\n            axes[1, 0].hist(self.processed_df[speed_col], bins=50, alpha=0.7, color='green')\n            axes[1, 0].set_xlabel('Speed (km/h)')\n            axes[1, 0].set_ylabel('Frequency')\n            axes[1, 0].set_title('Speed Distribution')\n            axes[1, 0].grid(True, alpha=0.3)\n        \n        # Vehicle count by area\n        unique_vehicles_per_cluster = self.processed_df.groupby('density_cluster')['randomized_id'].nunique()\n        axes[1, 1].bar(range(len(unique_vehicles_per_cluster)), \n                      unique_vehicles_per_cluster.values, color='purple', alpha=0.7)\n        axes[1, 1].set_xlabel('Area Cluster')\n        axes[1, 1].set_ylabel('Unique Vehicles')\n        axes[1, 1].set_title('Vehicle Distribution by Area')\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(f'{output_dir}/statistics_dashboard.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        print(f\"Visualizations saved to {output_dir}/\")\n\n    def generate_report(self):\n        \"\"\"Generate a comprehensive analysis report\"\"\"\n        print(\"Generating analysis report...\")\n        \n        report = {\n            'data_summary': {\n                'total_records': len(self.processed_df),\n                'unique_vehicles': self.processed_df['randomized_id'].nunique(),\n                'geographic_bounds': {\n                    'lat_min': self.processed_df['lat'].min(),\n                    'lat_max': self.processed_df['lat'].max(),\n                    'lng_min': self.processed_df['lng'].min(),\n                    'lng_max': self.processed_df['lng'].max()\n                }\n            },\n            'popular_routes': {\n                'total_route_clusters': len(self.routes) if self.routes else 0,\n                'top_5_routes': []\n            },\n            'tight_places': {\n                'total_congestion_areas': len(self.tight_places) if self.tight_places else 0,\n                'severity_breakdown': {},\n                'top_5_congestion_areas': []\n            }\n        }\n        \n        # Add popular routes details\n        if self.routes:\n            for i, (route_id, route_info) in enumerate(list(self.routes.items())[:5]):\n                report['popular_routes']['top_5_routes'].append({\n                    'route_id': route_id,\n                    'trip_count': route_info['route_count'],\n                    'popularity_percentage': route_info['popularity_score'],\n                    'avg_length_km': route_info['avg_route_length_m'] / 1000,\n                    'start_location': route_info['avg_start_point'],\n                    'end_location': route_info['avg_end_point']\n                })\n        \n        # Add tight places details\n        if self.tight_places:\n            severity_counts = {'High': 0, 'Medium': 0, 'Low': 0}\n            for place_info in self.tight_places.values():\n                severity_counts[place_info['severity']] += 1\n            \n            report['tight_places']['severity_breakdown'] = severity_counts\n            \n            for i, (place_id, place_info) in enumerate(list(self.tight_places.items())[:5]):\n                report['tight_places']['top_5_congestion_areas'].append({\n                    'area_id': place_id,\n                    'congestion_score': place_info['congestion_score'],\n                    'severity': place_info['severity'],\n                    'unique_vehicles': place_info['unique_vehicles'],\n                    'avg_speed_kmh': place_info['avg_speed_kmh'],\n                    'location': {\n                        'lat': place_info['center_lat'],\n                        'lng': place_info['center_lng']\n                    }\n                })\n        \n        return report\n\n\ndef run_complete_analysis(data_path_or_df, output_dir='./geo_analysis_output', sample_size=400000):\n    \"\"\"Run complete geo-tracking analysis pipeline focused on routes and congestion\"\"\"\n    print(\"=\"*60)\n    print(\"ADVANCED GEO-TRACKING ANALYSIS\")\n    print(\"FOCUS: Popular Routes & Congestion Areas\")\n    print(\"=\"*60)\n    \n    # Initialize analyzer with sampling\n    analyzer = AdvancedGeoTrackAnalyzer(data_path_or_df, sample_size=sample_size)\n    \n    # 1. Preprocess data\n    analyzer.preprocess_data()\n    \n    # 2. Identify popular routes\n    print(\"\\n\" + \"=\"*40)\n    print(\"IDENTIFYING POPULAR ROUTES\")\n    print(\"=\"*40)\n    routes = analyzer.identify_popular_routes()\n    \n    # 3. Identify tight places (congestion areas)\n    print(\"\\n\" + \"=\"*40)\n    print(\"IDENTIFYING CONGESTION AREAS\")\n    print(\"=\"*40)\n    tight_places = analyzer.identify_tight_places()\n    \n    # 4. Analyze route efficiency\n    print(\"\\n\" + \"=\"*40)\n    print(\"ANALYZING ROUTE EFFICIENCY\")\n    print(\"=\"*40)\n    efficiency = analyzer.analyze_route_efficiency()\n    \n    # 5. Create visualizations\n    print(\"\\n\" + \"=\"*40)\n    print(\"CREATING VISUALIZATIONS\")\n    print(\"=\"*40)\n    analyzer.create_visualizations(output_dir)\n    \n    # 6. Generate report\n    report = analyzer.generate_report()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ANALYSIS COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"Results saved to: {output_dir}\")\n    print(f\"Total records processed: {len(analyzer.processed_df):,}\")\n    print(f\"Unique vehicles: {analyzer.processed_df['randomized_id'].nunique():,}\")\n    print(f\"Popular routes identified: {len(routes)}\")\n    print(f\"Congestion areas identified: {len(tight_places)}\")\n    def convert_numpy_types(obj):\n        \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n        if isinstance(obj, dict):\n            return {str(k): convert_numpy_types(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [convert_numpy_types(item) for item in obj]\n        elif isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return obj\n    if routes:\n        print(f\"\\nTop 3 Popular Routes:\")\n        for i, (route_id, route_info) in enumerate(list(routes.items())[:3]):\n            print(f\"  Route {route_id}: {route_info['route_count']} trips ({route_info['popularity_score']:.1f}% of all routes)\")\n        with open(f'{output_dir}/popular_routes.json', 'w') as f:\n            json.dump(convert_numpy_types(routes), f, indent=2, default=str)\n        print(f\"Popular routes saved to {output_dir}/popular_routes.json\")\n    if tight_places:\n        print(f\"\\nTop 3 Congestion Areas:\")\n        for i, (place_id, place_info) in enumerate(list(tight_places.items())[:3]):\n            print(f\"  Area {place_id}: {place_info['severity']} severity, {place_info['unique_vehicles']} vehicles, avg speed {place_info['avg_speed_kmh']:.1f} km/h\")\n        with open(f'{output_dir}/tight_places.json', 'w') as f:\n            json.dump(convert_numpy_types(tight_places), f, indent=2, default=str)\n        print(f\"Tight places saved to {output_dir}/tight_places.json\")\n    return analyzer, report\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Run analysis on your dataset\n    analyzer, report = run_complete_analysis(\n        \"/kaggle/input/indrive-data/geo_locations_astana_hackathon\", \n        sample_size=500000\n    )\n    \n    # Print detailed summary\n    print(\"\\n=== DETAILED ANALYSIS SUMMARY ===\")\n    print(\"Popular Routes:\")\n    for route in report['popular_routes']['top_5_routes']:\n        print(f\"  - Route {route['route_id']}: {route['trip_count']} trips, {route['avg_length_km']:.2f} km avg length\")\n    \n    print(\"\\nCongestion Areas:\")\n    for area in report['tight_places']['top_5_congestion_areas']:\n        print(f\"  - Area {area['area_id']}: {area['severity']} severity, {area['unique_vehicles']} vehicles\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T14:08:10.094475Z","iopub.execute_input":"2025-09-13T14:08:10.095287Z","iopub.status.idle":"2025-09-13T14:09:05.883915Z","shell.execute_reply.started":"2025-09-13T14:08:10.095261Z","shell.execute_reply":"2025-09-13T14:09:05.882994Z"}},"outputs":[{"name":"stdout","text":"============================================================\nADVANCED GEO-TRACKING ANALYSIS\nFOCUS: Popular Routes & Congestion Areas\n============================================================\nLoading data from /kaggle/input/indrive-data/geo_locations_astana_hackathon\nOriginal dataset size: 1,262,687 rows\nAvailable columns: ['randomized_id', 'lat', 'lng', 'alt', 'spd', 'azm']\nSampling 500,000 rows from 1,262,687 total rows\nUsing sampled dataset of 500,000 rows\nPreprocessing data...\nSpeed data available: True\nAzimuth data available: True\nCreating derived features...\nPreprocessing complete. Final dataset: 500,000 rows\n\n========================================\nIDENTIFYING POPULAR ROUTES\n========================================\nIdentifying popular routes...\nExtracting trajectory start and end points...\nTotal trajectories: 6618\nTrajectories with ≥3 points: 6176\nDistance threshold: 0.000309 degrees\nRoutes after distance filtering: 5558\nRoute vectors shape: (5558, 4)\n\nTrying DBSCAN clustering...\n  eps=0.1: 73 clusters, 4815 noise points\n  eps=0.2: 54 clusters, 2577 noise points\n  eps=0.5: 6 clusters, 135 noise points\n  eps=1.0: 1 clusters, 0 noise points\n  eps=1.5: 1 clusters, 0 noise points\n  eps=2.0: 1 clusters, 0 noise points\nBest DBSCAN result: eps=0.1, 73 clusters\n\nSuccessfully identified 73 popular route clusters!\n  dbscan_5: 60 trips (1.1%)\n  dbscan_7: 58 trips (1.0%)\n  dbscan_0: 50 trips (0.9%)\n  dbscan_15: 44 trips (0.8%)\n  dbscan_13: 29 trips (0.5%)\n\n========================================\nIDENTIFYING CONGESTION AREAS\n========================================\nIdentifying tight places (congestion areas)...\nIdentified 1 tight places (congestion areas)\n\n========================================\nANALYZING ROUTE EFFICIENCY\n========================================\nAnalyzing route efficiency...\n\n========================================\nCREATING VISUALIZATIONS\n========================================\nCreating visualizations...\nVisualizations saved to ./geo_analysis_output/\nGenerating analysis report...\n\n============================================================\nANALYSIS COMPLETE!\n============================================================\nResults saved to: ./geo_analysis_output\nTotal records processed: 500,000\nUnique vehicles: 6,618\nPopular routes identified: 73\nCongestion areas identified: 1\n\nTop 3 Popular Routes:\n  Route dbscan_5: 60 trips (1.1% of all routes)\n  Route dbscan_7: 58 trips (1.0% of all routes)\n  Route dbscan_0: 50 trips (0.9% of all routes)\nPopular routes saved to ./geo_analysis_output/popular_routes.json\n\nTop 3 Congestion Areas:\n  Area 4: High severity, 4 vehicles, avg speed 0.5 km/h\nTight places saved to ./geo_analysis_output/tight_places.json\n\n=== DETAILED ANALYSIS SUMMARY ===\nPopular Routes:\n  - Route dbscan_5: 60 trips, 0.15 km avg length\n  - Route dbscan_7: 58 trips, 0.12 km avg length\n  - Route dbscan_0: 50 trips, 0.10 km avg length\n  - Route dbscan_15: 44 trips, 0.14 km avg length\n  - Route dbscan_13: 29 trips, 0.09 km avg length\n\nCongestion Areas:\n  - Area 4: High severity, 4 vehicles\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}